{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, TimeDistributed, Conv2D, MaxPooling1D, Input, concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras import backend as K\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Tick Data and Create 5min RTH Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:52.612000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:52.615000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:54.157000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:54.157000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:55.332000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    last      bid     ask  volume\n",
       "date                                                             \n",
       "2018-01-25 19:54:52.612000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:52.615000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:54.157000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:54.157000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:55.332000-05:00  2844.0  2843.75  2844.0       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tick_data = pd.read_feather('../data/processed/ES_tick.feather')\n",
    "tick_data = tick_data[tick_data['date'] > '2017-07-29']\n",
    "#Create Index from date column\n",
    "tick_data.index = tick_data['date']\n",
    "tick_data.drop(labels=['date'],axis=1,inplace=True)\n",
    "tick_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2017-08-01 09:35:00-04:00</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>2476.00</td>\n",
       "      <td>2472.50</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2470.908594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2017-08-01 09:40:00-04:00</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2471.50</td>\n",
       "      <td>2472.50</td>\n",
       "      <td>2471.060194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2017-08-01 09:45:00-04:00</td>\n",
       "      <td>2472.25</td>\n",
       "      <td>2473.25</td>\n",
       "      <td>2471.75</td>\n",
       "      <td>2473.00</td>\n",
       "      <td>2471.244978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-08-01 09:50:00-04:00</td>\n",
       "      <td>2473.00</td>\n",
       "      <td>2473.25</td>\n",
       "      <td>2472.00</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2471.388343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-08-01 09:55:00-04:00</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2473.00</td>\n",
       "      <td>2471.25</td>\n",
       "      <td>2471.25</td>\n",
       "      <td>2471.375165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date     open     high      low    close          ema\n",
       "81 2017-08-01 09:35:00-04:00  2475.50  2476.00  2472.50  2473.50  2470.908594\n",
       "82 2017-08-01 09:40:00-04:00  2473.50  2474.00  2471.50  2472.50  2471.060194\n",
       "83 2017-08-01 09:45:00-04:00  2472.25  2473.25  2471.75  2473.00  2471.244978\n",
       "84 2017-08-01 09:50:00-04:00  2473.00  2473.25  2472.00  2472.75  2471.388343\n",
       "85 2017-08-01 09:55:00-04:00  2472.75  2473.00  2471.25  2471.25  2471.375165"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resample to get 5min bars\n",
    "five_min_data = pd.DataFrame(\n",
    "    tick_data['last'].resample('5Min', loffset=datetime.timedelta(minutes=5)).ohlc())\n",
    "\n",
    "import pandas_market_calendars as mcal\n",
    "#We hack the NYSE Calendar extending the close until 4:15\n",
    "class CMERTHCalendar(mcal.exchange_calendar_nyse.NYSEExchangeCalendar):\n",
    "    @property\n",
    "    def close_time(self):\n",
    "        return datetime.time(16, 15)\n",
    "    \n",
    "#Create RTH Calendar\n",
    "nyse = CMERTHCalendar()\n",
    "schedule = nyse.schedule(start_date=five_min_data.index.min(), \n",
    "                         end_date=five_min_data.index.max())\n",
    "\n",
    "#Filter out those bars that occur during RTH\n",
    "five_min_data['dates'] = pd.to_datetime(five_min_data.index.to_datetime().date)\n",
    "five_min_data['valid_date'] = five_min_data['dates'].isin(schedule.index)\n",
    "five_min_data['valid_time'] = False\n",
    "during_rth = five_min_data['valid_date'] & \\\n",
    "        (five_min_data.index > schedule.loc[five_min_data['dates'],'market_open']) & \\\n",
    "        (five_min_data.index <= schedule.loc[five_min_data['dates'],'market_close'])\n",
    "five_min_data.loc[during_rth, 'valid_time'] = True\n",
    "five_min_data = five_min_data[five_min_data['valid_time'] == True]\n",
    "five_min_data.drop(['dates','valid_date','valid_time'], axis=1, inplace=True)\n",
    "\n",
    "#Add ema\n",
    "five_min_data['ema'] = five_min_data['close'].ewm(span=20, min_periods=20).mean()\n",
    "\n",
    "#Reset index\n",
    "five_min_data.reset_index(inplace=True)\n",
    "\n",
    "five_min_data[81:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-31 09:35:00-04:00</td>\n",
       "      <td>2474.75</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.402747</td>\n",
       "      <td>-0.915311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-31 09:40:00-04:00</td>\n",
       "      <td>2475.25</td>\n",
       "      <td>2476.00</td>\n",
       "      <td>2473.75</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.422618</td>\n",
       "      <td>-0.906308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-31 09:45:00-04:00</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2474.50</td>\n",
       "      <td>2474.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.442289</td>\n",
       "      <td>-0.896873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-31 09:50:00-04:00</td>\n",
       "      <td>2474.50</td>\n",
       "      <td>2475.00</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2473.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.461749</td>\n",
       "      <td>-0.887011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-31 09:55:00-04:00</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2474.25</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.480989</td>\n",
       "      <td>-0.876727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date     open     high      low    close  ema  \\\n",
       "0 2017-07-31 09:35:00-04:00  2474.75  2475.75  2474.00  2475.50  NaN   \n",
       "1 2017-07-31 09:40:00-04:00  2475.25  2476.00  2473.75  2475.50  NaN   \n",
       "2 2017-07-31 09:45:00-04:00  2475.75  2475.75  2474.50  2474.75  NaN   \n",
       "3 2017-07-31 09:50:00-04:00  2474.50  2475.00  2473.50  2473.75  NaN   \n",
       "4 2017-07-31 09:55:00-04:00  2474.00  2474.25  2472.75  2472.75  NaN   \n",
       "\n",
       "   sin_time  cos_time  \n",
       "0 -0.402747 -0.915311  \n",
       "1 -0.422618 -0.906308  \n",
       "2 -0.442289 -0.896873  \n",
       "3 -0.461749 -0.887011  \n",
       "4 -0.480989 -0.876727  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add column for number of seconds elapsed in trading day\n",
    "five_min_data['sec'] = (five_min_data['date'].values \n",
    "                        - five_min_data['date'].values.astype('datetime64[D]')) / np.timedelta64(1,'s')\n",
    "\n",
    "#Calculate sin & cos time\n",
    "#24hr time is a cyclical continuous feature\n",
    "seconds_in_day = 24*60*60\n",
    "five_min_data['sin_time'] = np.sin(2*np.pi*five_min_data['sec']/seconds_in_day)\n",
    "five_min_data['cos_time'] = np.cos(2*np.pi*five_min_data['sec']/seconds_in_day)\n",
    "\n",
    "five_min_data.drop('sec', axis=1, inplace=True)\n",
    "five_min_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test / Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "      <th>btc</th>\n",
       "      <th>stc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-31 09:35:00-04:00</td>\n",
       "      <td>2474.75</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.402747</td>\n",
       "      <td>-0.915311</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-31 09:40:00-04:00</td>\n",
       "      <td>2475.25</td>\n",
       "      <td>2476.00</td>\n",
       "      <td>2473.75</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.422618</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-31 09:45:00-04:00</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2474.50</td>\n",
       "      <td>2474.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.442289</td>\n",
       "      <td>-0.896873</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-31 09:50:00-04:00</td>\n",
       "      <td>2474.50</td>\n",
       "      <td>2475.00</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2473.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.461749</td>\n",
       "      <td>-0.887011</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-31 09:55:00-04:00</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2474.25</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.480989</td>\n",
       "      <td>-0.876727</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date     open     high      low    close  ema  \\\n",
       "0 2017-07-31 09:35:00-04:00  2474.75  2475.75  2474.00  2475.50  NaN   \n",
       "1 2017-07-31 09:40:00-04:00  2475.25  2476.00  2473.75  2475.50  NaN   \n",
       "2 2017-07-31 09:45:00-04:00  2475.75  2475.75  2474.50  2474.75  NaN   \n",
       "3 2017-07-31 09:50:00-04:00  2474.50  2475.00  2473.50  2473.75  NaN   \n",
       "4 2017-07-31 09:55:00-04:00  2474.00  2474.25  2472.75  2472.75  NaN   \n",
       "\n",
       "   sin_time  cos_time  btc  stc  \n",
       "0 -0.402747 -0.915311 -104   96  \n",
       "1 -0.422618 -0.906308 -104   96  \n",
       "2 -0.442289 -0.896873 -104   96  \n",
       "3 -0.461749 -0.887011 -104   96  \n",
       "4 -0.480989 -0.876727 -104   96  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_min_data = pd.read_feather('../data/processed/ES_TFCnn.feather')\n",
    "five_min_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "      <th>btc</th>\n",
       "      <th>stc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-03 09:35:00-05:00</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1260.75</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.625923</td>\n",
       "      <td>-0.779884</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-03 09:40:00-05:00</td>\n",
       "      <td>1259.50</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>1259.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-03 09:45:00-05:00</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>1260.25</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>1260.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.659346</td>\n",
       "      <td>-0.751840</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-03 09:50:00-05:00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1260.00</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>1258.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.675590</td>\n",
       "      <td>-0.737277</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-03 09:55:00-05:00</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1257.50</td>\n",
       "      <td>1257.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.691513</td>\n",
       "      <td>-0.722364</td>\n",
       "      <td>96</td>\n",
       "      <td>-104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date     open     high      low    close  ema  \\\n",
       "0 2006-01-03 09:35:00-05:00  1259.00  1260.75  1259.00  1259.75  NaN   \n",
       "1 2006-01-03 09:40:00-05:00  1259.50  1259.75  1258.50  1259.50  NaN   \n",
       "2 2006-01-03 09:45:00-05:00  1259.25  1260.25  1259.25  1260.00  NaN   \n",
       "3 2006-01-03 09:50:00-05:00  1259.75  1260.00  1258.50  1258.75  NaN   \n",
       "4 2006-01-03 09:55:00-05:00  1259.00  1259.75  1257.50  1257.75  NaN   \n",
       "\n",
       "   sin_time  cos_time  btc  stc  \n",
       "0 -0.625923 -0.779884 -104   96  \n",
       "1 -0.642788 -0.766044 -104   96  \n",
       "2 -0.659346 -0.751840 -104   96  \n",
       "3 -0.675590 -0.737277 -104   96  \n",
       "4 -0.691513 -0.722364   96 -104  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_min_data = pd.read_hdf('../data/processed/store.h5', key='cnn_data')\n",
    "five_min_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "      <th>btc</th>\n",
       "      <th>stc</th>\n",
       "      <th>change</th>\n",
       "      <th>cdl_sign</th>\n",
       "      <th>cdl_body</th>\n",
       "      <th>cdl_ut</th>\n",
       "      <th>cdl_lt</th>\n",
       "      <th>cdl_rng</th>\n",
       "      <th>cdl_hl</th>\n",
       "      <th>cdl_lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-03 09:35:00-05:00</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1260.75</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.625923</td>\n",
       "      <td>-0.779884</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-03 09:40:00-05:00</td>\n",
       "      <td>1259.50</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>1259.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-03 09:45:00-05:00</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>1260.25</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>1260.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.659346</td>\n",
       "      <td>-0.751840</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-03 09:50:00-05:00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1260.00</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>1258.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.675590</td>\n",
       "      <td>-0.737277</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-03 09:55:00-05:00</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1257.50</td>\n",
       "      <td>1257.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.691513</td>\n",
       "      <td>-0.722364</td>\n",
       "      <td>96</td>\n",
       "      <td>-104</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date     open     high      low    close  ema  \\\n",
       "0 2006-01-03 09:35:00-05:00  1259.00  1260.75  1259.00  1259.75  NaN   \n",
       "1 2006-01-03 09:40:00-05:00  1259.50  1259.75  1258.50  1259.50  NaN   \n",
       "2 2006-01-03 09:45:00-05:00  1259.25  1260.25  1259.25  1260.00  NaN   \n",
       "3 2006-01-03 09:50:00-05:00  1259.75  1260.00  1258.50  1258.75  NaN   \n",
       "4 2006-01-03 09:55:00-05:00  1259.00  1259.75  1257.50  1257.75  NaN   \n",
       "\n",
       "   sin_time  cos_time  btc  stc  change  cdl_sign  cdl_body  cdl_ut  cdl_lt  \\\n",
       "0 -0.625923 -0.779884 -104   96     NaN       1.0      0.75    1.00    0.00   \n",
       "1 -0.642788 -0.766044 -104   96   -0.25       0.0      0.00    0.25    1.00   \n",
       "2 -0.659346 -0.751840 -104   96    0.50       1.0      0.75    0.25    0.00   \n",
       "3 -0.675590 -0.737277 -104   96   -1.25      -1.0      1.00    0.25    0.25   \n",
       "4 -0.691513 -0.722364   96 -104   -1.00      -1.0      1.25    0.75    0.25   \n",
       "\n",
       "   cdl_rng  cdl_hl  cdl_lh  \n",
       "0     1.75       0       0  \n",
       "1     1.25       0       1  \n",
       "2     1.00       1       0  \n",
       "3     1.50       0       1  \n",
       "4     2.25       0       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = five_min_data\n",
    "fd['change'] = fd['close'] - fd['close'].shift(1)\n",
    "fd['cdl_sign'] = np.sign(fd['close'] - fd['open'])\n",
    "fd['cdl_body'] = np.absolute(fd['close'] - fd['open'])\n",
    "#fd['cdl_ut'] = fd['high'] - fd['close'] if fd['cdl_body'] > 0 else fd['high'] - fd['open']\n",
    "fd['cdl_ut'] = np.where(fd['cdl_sign'] > 0, fd['high'] - fd['close'], fd['high'] - fd['open'])\n",
    "fd['cdl_lt'] = np.where(fd['cdl_sign'] > 0, fd['open'] - fd['low'], fd['close'] - fd['low'])\n",
    "fd['cdl_rng'] = fd['high'] - fd['low']\n",
    "fd['cdl_hl'] = np.where(fd['low'] >= fd['low'].shift(), 1, 0) #higher low\n",
    "fd['cdl_lh'] = np.where(fd['high'] <= fd['high'].shift(), 1, 0) #lower high\n",
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "      <th>btc</th>\n",
       "      <th>stc</th>\n",
       "      <th>change</th>\n",
       "      <th>cdl_sign</th>\n",
       "      <th>cdl_body</th>\n",
       "      <th>cdl_ut</th>\n",
       "      <th>cdl_lt</th>\n",
       "      <th>cdl_rng</th>\n",
       "      <th>cdl_hl</th>\n",
       "      <th>cdl_lh</th>\n",
       "      <th>pivot_high</th>\n",
       "      <th>pivot_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-03 09:35:00-05:00</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1260.75</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.625923</td>\n",
       "      <td>-0.779884</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-03 09:40:00-05:00</td>\n",
       "      <td>1259.50</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>1259.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-03 09:45:00-05:00</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>1260.25</td>\n",
       "      <td>1259.25</td>\n",
       "      <td>1260.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.659346</td>\n",
       "      <td>-0.751840</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-03 09:50:00-05:00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1260.00</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>1258.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.675590</td>\n",
       "      <td>-0.737277</td>\n",
       "      <td>-104</td>\n",
       "      <td>96</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-03 09:55:00-05:00</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1259.75</td>\n",
       "      <td>1257.50</td>\n",
       "      <td>1257.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.691513</td>\n",
       "      <td>-0.722364</td>\n",
       "      <td>96</td>\n",
       "      <td>-104</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date     open     high      low    close  ema  \\\n",
       "0 2006-01-03 09:35:00-05:00  1259.00  1260.75  1259.00  1259.75  NaN   \n",
       "1 2006-01-03 09:40:00-05:00  1259.50  1259.75  1258.50  1259.50  NaN   \n",
       "2 2006-01-03 09:45:00-05:00  1259.25  1260.25  1259.25  1260.00  NaN   \n",
       "3 2006-01-03 09:50:00-05:00  1259.75  1260.00  1258.50  1258.75  NaN   \n",
       "4 2006-01-03 09:55:00-05:00  1259.00  1259.75  1257.50  1257.75  NaN   \n",
       "\n",
       "   sin_time  cos_time  btc  stc  change  cdl_sign  cdl_body  cdl_ut  cdl_lt  \\\n",
       "0 -0.625923 -0.779884 -104   96     NaN       1.0      0.75    1.00    0.00   \n",
       "1 -0.642788 -0.766044 -104   96   -0.25       0.0      0.00    0.25    1.00   \n",
       "2 -0.659346 -0.751840 -104   96    0.50       1.0      0.75    0.25    0.00   \n",
       "3 -0.675590 -0.737277 -104   96   -1.25      -1.0      1.00    0.25    0.25   \n",
       "4 -0.691513 -0.722364   96 -104   -1.00      -1.0      1.25    0.75    0.25   \n",
       "\n",
       "   cdl_rng  cdl_hl  cdl_lh  pivot_high  pivot_low  \n",
       "0     1.75       0       0       False      False  \n",
       "1     1.25       0       1       False       True  \n",
       "2     1.00       1       0        True      False  \n",
       "3     1.50       0       1       False      False  \n",
       "4     2.25       0       1       False      False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "higher_than_next_bar = fd['high'] > fd.shift(-1)['high']\n",
    "higher_than_prev_bar = fd['high'] > fd.shift(1)['high']\n",
    "lower_than_next_bar = fd['low'] < fd.shift(-1)['low']\n",
    "lower_than_prev_bar = fd['low'] < fd.shift(1)['low']\n",
    "fd['pivot_high'] = higher_than_next_bar & higher_than_prev_bar\n",
    "fd['pivot_low'] = lower_than_next_bar & lower_than_prev_bar\n",
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = fd[81:]\n",
    "\n",
    "openp = data['open'].tolist()\n",
    "highp = data['high'].tolist()\n",
    "lowp = data['low'].tolist()\n",
    "closep = data['close'].tolist()\n",
    "emap = data['ema'].tolist()\n",
    "sin_time = data['sin_time'].tolist()\n",
    "cos_time = data['cos_time'].tolist()\n",
    "btc = data['btc'].tolist()\n",
    "stc = data['stc'].tolist()\n",
    "\n",
    "change = data['change'].tolist()\n",
    "\n",
    "cdl_sign = data['cdl_sign'].tolist()\n",
    "cdl_body = data['cdl_body'].tolist()\n",
    "cdl_ut = data['cdl_ut'].tolist()\n",
    "cdl_lt = data['cdl_lt'].tolist()\n",
    "cdl_rng = data['cdl_rng'].tolist()\n",
    "cdl_hl = data['cdl_hl'].tolist()\n",
    "cdl_lh = data['cdl_lh'].tolist()\n",
    "\n",
    "pivot_high = data['pivot_high'].astype('int').tolist()\n",
    "pivot_low = data['pivot_low'].astype('int').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "closep = (data['close'].shift(-1) - data['close']).tolist()[:-1]\n",
    "btc = data['btc'].shift(-1).tolist()[:-1]\n",
    "sin_time = data['sin_time'].shift(-1).tolist()[:-1]\n",
    "cos_time = data['cos_time'].shift(-1).tolist()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1288.2096, 21.484784856730588)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = int(data.shape[0] * 0.9)\n",
    "p = 10000\n",
    "mean = data.mean(axis=0)\n",
    "std = data.std(axis=0)\n",
    "mean_c = np.mean(closep[0:p])\n",
    "std_c = np.std(closep[0:p])\n",
    "mean_c, std_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW = 36 #Number of bars in a trading day\n",
    "EMB_SIZE = 9\n",
    "STEP = 1\n",
    "FORECAST = 1\n",
    "\n",
    "X, Y = [], []\n",
    "for i in range(0, len(data)-WINDOW+1, STEP):\n",
    "    try:\n",
    "        o = openp[i:i+WINDOW]\n",
    "        h = highp[i:i+WINDOW]\n",
    "        l = lowp[i:i+WINDOW]\n",
    "        c = closep[i:i+WINDOW]\n",
    "        e = emap[i:i+WINDOW]\n",
    "        ct = cos_time[i:i+WINDOW]\n",
    "        st = sin_time[i:i+WINDOW]\n",
    "        \n",
    "        cng = change[i:i+WINDOW]\n",
    "        \n",
    "        _cdl_sign = cdl_sign[i:i+WINDOW]\n",
    "        _cdl_body = cdl_body[i:i+WINDOW]\n",
    "        _cdl_ut = cdl_ut[i:i+WINDOW]\n",
    "        _cdl_lt = cdl_lt[i:i+WINDOW]\n",
    "        _cdl_rng = cdl_rng[i:i+WINDOW]\n",
    "        _cdl_hl = cdl_hl[i:i+WINDOW]\n",
    "        _cdl_lh = cdl_lh[i:i+WINDOW]\n",
    "        \n",
    "        _pivot_high = np.array(pivot_high[i:i+WINDOW]) * np.array(h)\n",
    "        _pivot_low = np.array(pivot_low[i:i+WINDOW]) * np.array(l)\n",
    "        \n",
    "        o = (np.array(o) - np.mean(o)) / np.std(o)\n",
    "        h = (np.array(h) - np.mean(h)) / np.std(h)\n",
    "        l = (np.array(l) - np.mean(l)) / np.std(l)\n",
    "        c = (np.array(c) - np.mean(c)) / np.std(c)\n",
    "        e = (np.array(e) - np.mean(e)) / np.std(e)\n",
    "        \n",
    "        ph = (np.array(_pivot_high) - np.mean(h)) / np.std(h)\n",
    "        pl = (np.array(_pivot_low) - np.mean(l)) / np.std(l)\n",
    "        \n",
    "        _cng = (np.array(cng) - np.mean(cng)) / np.std(cng)\n",
    "        \n",
    "        \n",
    "        #c = (np.array(c) - mean_c) / std_c\n",
    "        \n",
    "        #o = np.divide(o, c[-1]) \n",
    "        #h = np.divide(h, c[-1])\n",
    "        #l = np.divide(l, c[-1])\n",
    "        #e = np.divide(e, c[-1])\n",
    "        #c = np.divide(c, c[-1])\n",
    "\n",
    "        x_i = closep[i:i+WINDOW]\n",
    "        y_i = closep[(i+WINDOW-1)+FORECAST]  \n",
    "\n",
    "        last_close = x_i[-1]\n",
    "        next_close = y_i\n",
    "\n",
    "        if btc[i+WINDOW-1] > 0:\n",
    "            y_i = [1, 0]\n",
    "        else:\n",
    "            y_i = [0, 1]\n",
    "        \n",
    "        x_i = np.column_stack((o, h, l, c, e, ct, st, cng, _cdl_hl))\n",
    "        #x_i = np.column_stack((cng))\n",
    "        #x_i = np.column_stack((c))\n",
    "        \n",
    "    except Exception as e:\n",
    "        #e.throw()\n",
    "        break\n",
    "\n",
    "    #only add if 1pt body and close on high\n",
    "    if (closep[i+WINDOW-1] == highp[i+WINDOW-1]) and (closep[i+WINDOW-1]-openp[i+WINDOW-1]>=1):\n",
    "        X.append(x_i)\n",
    "        Y.append(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99765046,  0.9986445 ,  0.99746973,  0.99855413,  0.99779185,\n",
       "        -0.70710678, -0.70710678,  2.5       ,  1.        ],\n",
       "       [ 0.99855413,  0.99900596,  0.99828303,  0.99882523,  0.99789027,\n",
       "        -0.69151306, -0.72236396,  0.75      ,  1.        ],\n",
       "       [ 0.99882523,  0.99909633,  0.9983734 ,  0.99882523,  0.99797931,\n",
       "        -0.67559021, -0.73727734,  0.        ,  1.        ],\n",
       "       [ 0.9989156 ,  0.99900596,  0.9983734 ,  0.99855413,  0.99803405,\n",
       "        -0.65934582, -0.75183981, -0.75      ,  1.        ],\n",
       "       [ 0.99855413,  0.99873486,  0.9983734 ,  0.9986445 ,  0.99809219,\n",
       "        -0.64278761, -0.76604444,  0.25      ,  1.        ],\n",
       "       [ 0.9986445 ,  0.99882523,  0.99746973,  0.99746973,  0.99803291,\n",
       "        -0.62592347, -0.77988448, -3.25      ,  0.        ],\n",
       "       [ 0.99746973,  0.99882523,  0.99746973,  0.99882523,  0.99810837,\n",
       "        -0.60876143, -0.79335334,  3.75      ,  1.        ],\n",
       "       [ 0.99882523,  0.99900596,  0.99846376,  0.99882523,  0.99817664,\n",
       "        -0.59130965, -0.8064446 ,  0.        ,  1.        ],\n",
       "       [ 0.99882523,  0.99909633,  0.99873486,  0.99900596,  0.99825562,\n",
       "        -0.57357644, -0.81915204,  0.5       ,  1.        ],\n",
       "       [ 0.99900596,  0.99963853,  0.99882523,  0.99963853,  0.99838733,\n",
       "        -0.55557023, -0.83146961,  1.75      ,  1.        ],\n",
       "       [ 0.99963853,  0.99963853,  0.99927706,  0.9994578 ,  0.99848928,\n",
       "        -0.53729961, -0.84339145, -0.5       ,  1.        ],\n",
       "       [ 0.9994578 ,  0.99963853,  0.99927706,  0.99927706,  0.99856431,\n",
       "        -0.51877326, -0.85491187, -0.5       ,  1.        ],\n",
       "       [ 0.99927706,  0.99936743,  0.9989156 ,  0.99900596,  0.99860637,\n",
       "        -0.5       , -0.8660254 , -0.75      ,  0.        ],\n",
       "       [ 0.9989156 ,  0.99927706,  0.99873486,  0.9991867 ,  0.99866164,\n",
       "        -0.48098877, -0.87672676,  0.5       ,  0.        ],\n",
       "       [ 0.9991867 ,  0.9994578 ,  0.99909633,  0.99927706,  0.99872025,\n",
       "        -0.46174861, -0.88701083,  0.25      ,  1.        ],\n",
       "       [ 0.99927706,  0.9997289 ,  0.99909633,  0.9997289 ,  0.99881631,\n",
       "        -0.44228869, -0.89687274,  1.25      ,  1.        ],\n",
       "       [ 0.9997289 ,  1.00009037,  0.99927706,  0.99927706,  0.99886019,\n",
       "        -0.42261826, -0.90630779, -1.25      ,  1.        ],\n",
       "       [ 0.99936743,  0.99936743,  0.99900596,  0.99900596,  0.99887408,\n",
       "        -0.40274669, -0.91531148, -0.75      ,  0.        ],\n",
       "       [ 0.99900596,  0.99927706,  0.9989156 ,  0.99909633,  0.99889524,\n",
       "        -0.38268343, -0.92387953,  0.25      ,  0.        ],\n",
       "       [ 0.99900596,  0.99909633,  0.99873486,  0.99882523,  0.99888858,\n",
       "        -0.36243804, -0.93200787, -0.75      ,  0.        ],\n",
       "       [ 0.99882523,  0.9989156 ,  0.99855413,  0.99873486,  0.99887394,\n",
       "        -0.34202014, -0.93969262, -0.25      ,  0.        ],\n",
       "       [ 0.9986445 ,  0.99900596,  0.99855413,  0.99900596,  0.99888651,\n",
       "        -0.32143947, -0.94693013,  0.75      ,  1.        ],\n",
       "       [ 0.99900596,  0.99909633,  0.9986445 ,  0.99882523,  0.99888067,\n",
       "        -0.3007058 , -0.95371695, -0.5       ,  1.        ],\n",
       "       [ 0.99882523,  0.99936743,  0.99882523,  0.99882523,  0.99887539,\n",
       "        -0.27982901, -0.96004985,  0.        ,  1.        ],\n",
       "       [ 0.9989156 ,  0.9991867 ,  0.9986445 ,  0.99900596,  0.99888783,\n",
       "        -0.25881905, -0.96592583,  0.5       ,  0.        ],\n",
       "       [ 0.99909633,  0.99936743,  0.99900596,  0.9991867 ,  0.99891629,\n",
       "        -0.23768589, -0.97134207,  0.5       ,  1.        ],\n",
       "       [ 0.99927706,  0.99954817,  0.9991867 ,  0.99936743,  0.99895926,\n",
       "        -0.21643961, -0.97629601,  0.5       ,  1.        ],\n",
       "       [ 0.99936743,  0.99954817,  0.9991867 ,  0.99936743,  0.99899813,\n",
       "        -0.19509032, -0.98078528,  0.        ,  1.        ],\n",
       "       [ 0.9994578 ,  0.99954817,  0.99909633,  0.99909633,  0.99900748,\n",
       "        -0.17364818, -0.98480775, -0.75      ,  0.        ],\n",
       "       [ 0.9991867 ,  0.9994578 ,  0.99909633,  0.9991867 ,  0.99902455,\n",
       "        -0.15212339, -0.98836151,  0.25      ,  1.        ],\n",
       "       [ 0.99927706,  0.99963853,  0.99909633,  0.99954817,  0.99907442,\n",
       "        -0.13052619, -0.99144486,  1.        ,  1.        ],\n",
       "       [ 0.99963853,  0.9997289 ,  0.99909633,  0.99927706,  0.99909372,\n",
       "        -0.10886687, -0.99405634, -0.75      ,  1.        ],\n",
       "       [ 0.99936743,  0.99954817,  0.9991867 ,  0.99954817,  0.999137  ,\n",
       "        -0.08715574, -0.9961947 ,  0.75      ,  1.        ],\n",
       "       [ 0.99954817,  1.        ,  0.9994578 ,  0.99963853,  0.99918477,\n",
       "        -0.06540313, -0.99785892,  0.25      ,  1.        ],\n",
       "       [ 0.99963853,  0.99981927,  0.9994578 ,  0.9997289 ,  0.99923659,\n",
       "        -0.04361939, -0.99904822,  0.25      ,  1.        ],\n",
       "       [ 0.99963853,  1.        ,  0.99963853,  1.        ,  0.99930929,\n",
       "        -0.02181489, -0.99976203,  0.75      ,  1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1, [1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.80902677e-01, -5.47949030e-01, -4.35377418e-01,\n",
       "        -6.73983911e-01, -3.13624790e-01, -1.30526192e-01,\n",
       "        -9.91444861e-01, -2.50000000e-01,  0.00000000e+00],\n",
       "       [-9.33321799e-01, -5.47949030e-01, -4.35377418e-01,\n",
       "        -3.66852002e-01, -3.03958998e-01, -1.08866875e-01,\n",
       "        -9.94056338e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [-2.80902677e-01, -5.47949030e-01, -8.70754836e-02,\n",
       "        -5.97200934e-02, -2.25056999e-01, -8.71557427e-02,\n",
       "        -9.96194698e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [ 4.53068834e-02, -5.47949030e-01, -4.35377418e-01,\n",
       "        -9.81115820e-01, -3.64139751e-01, -6.54031292e-02,\n",
       "        -9.97858923e-01, -7.50000000e-01,  0.00000000e+00],\n",
       "       [-9.33321799e-01, -8.46830318e-01, -4.35377418e-01,\n",
       "        -6.73983911e-01, -4.19819769e-01, -4.36193874e-02,\n",
       "        -9.99048222e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [-6.07112238e-01, -5.47949030e-01, -8.70754836e-02,\n",
       "        -3.66852002e-01, -4.00040169e-01, -2.18148850e-02,\n",
       "        -9.99762027e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [-2.80902677e-01, -2.49067741e-01, -8.70754836e-02,\n",
       "         2.47411815e-01, -2.41830824e-01, -1.83697020e-16,\n",
       "        -1.00000000e+00,  5.00000000e-01,  1.00000000e+00],\n",
       "       [ 3.71516444e-01,  4.98135481e-02,  6.09528385e-01,\n",
       "         2.47411815e-01, -9.86890360e-02,  2.18148850e-02,\n",
       "        -9.99762027e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 3.71516444e-01,  6.47576126e-01,  9.57830320e-01,\n",
       "         2.47411815e-01,  3.08202009e-02,  4.36193874e-02,\n",
       "        -9.99048222e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 3.71516444e-01,  3.48694837e-01, -8.70754836e-02,\n",
       "        -6.73983911e-01, -6.24750499e-02,  6.54031292e-02,\n",
       "        -9.97858923e-01, -7.50000000e-01,  0.00000000e+00],\n",
       "       [-9.33321799e-01, -1.14571161e+00, -1.13198129e+00,\n",
       "        -1.28824773e+00, -2.87198555e-01,  8.71557427e-02,\n",
       "        -9.96194698e-01, -5.00000000e-01,  0.00000000e+00],\n",
       "       [-1.25953136e+00, -1.44459290e+00, -1.13198129e+00,\n",
       "        -9.81115820e-01, -4.20363064e-01,  1.08866875e-01,\n",
       "        -9.94056338e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [-9.33321799e-01, -1.14571161e+00, -7.83679353e-01,\n",
       "        -9.81115820e-01, -5.40845238e-01,  1.30526192e-01,\n",
       "        -9.91444861e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [-9.33321799e-01, -8.46830318e-01, -7.83679353e-01,\n",
       "        -9.81115820e-01, -6.49852920e-01,  1.52123386e-01,\n",
       "        -9.88361510e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [-6.07112238e-01, -5.47949030e-01, -8.70754836e-02,\n",
       "        -6.73983911e-01, -6.78322159e-01,  1.73648178e-01,\n",
       "        -9.84807753e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [-2.80902677e-01, -8.46830318e-01, -1.48028322e+00,\n",
       "        -1.90251155e+00, -9.84707075e-01,  1.95090322e-01,\n",
       "        -9.80785280e-01, -1.00000000e+00,  0.00000000e+00],\n",
       "       [-1.58574092e+00, -1.44459290e+00, -1.48028322e+00,\n",
       "        -1.28824773e+00, -1.12159896e+00,  2.16439614e-01,\n",
       "        -9.76296007e-01,  5.00000000e-01,  1.00000000e+00],\n",
       "       [-1.25953136e+00, -1.44459290e+00, -1.13198129e+00,\n",
       "        -1.28824773e+00, -1.24545352e+00,  2.37685892e-01,\n",
       "        -9.71342070e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [-9.33321799e-01, -5.47949030e-01, -1.13198129e+00,\n",
       "        -3.66852002e-01, -1.14704214e+00,  2.58819045e-01,\n",
       "        -9.65925826e-01,  7.50000000e-01,  1.00000000e+00],\n",
       "       [ 4.53068834e-02, -2.49067741e-01, -8.70754836e-02,\n",
       "        -6.73983911e-01, -1.12816002e+00,  2.79829014e-01,\n",
       "        -9.60049854e-01, -2.50000000e-01,  1.00000000e+00],\n",
       "       [-2.80902677e-01, -8.46830318e-01, -1.13198129e+00,\n",
       "        -3.66852002e-01, -1.04091944e+00,  3.00705800e-01,\n",
       "        -9.53716951e-01,  2.50000000e-01,  0.00000000e+00],\n",
       "       [-2.80902677e-01, -2.49067741e-01,  2.61226451e-01,\n",
       "        -3.66852002e-01, -9.61987495e-01,  3.21439465e-01,\n",
       "        -9.46930129e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 4.53068834e-02, -5.47949030e-01, -4.35377418e-01,\n",
       "        -3.66852002e-01, -8.90572873e-01,  3.42020143e-01,\n",
       "        -9.39692621e-01,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-6.07112238e-01,  6.47576126e-01, -4.35377418e-01,\n",
       "         8.61675633e-01, -5.45332612e-01,  3.62438038e-01,\n",
       "        -9.32007869e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.02393557e+00,  9.46457415e-01,  6.09528385e-01,\n",
       "         5.54543724e-01, -3.03129133e-01,  3.82683432e-01,\n",
       "        -9.23879533e-01, -2.50000000e-01,  1.00000000e+00],\n",
       "       [ 3.71516444e-01,  6.47576126e-01,  9.57830320e-01,\n",
       "         5.54543724e-01, -8.39926527e-02,  4.02746690e-01,\n",
       "        -9.15311479e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 6.97726005e-01,  1.84310128e+00,  1.30613225e+00,\n",
       "         2.09020327e+00,  4.65057478e-01,  4.22618262e-01,\n",
       "        -9.06307787e-01,  1.25000000e+00,  1.00000000e+00],\n",
       "       [ 2.32877381e+00,  1.54421999e+00,  1.30613225e+00,\n",
       "         1.16880754e+00,  7.51346845e-01,  4.42288690e-01,\n",
       "        -8.96872742e-01, -7.50000000e-01,  1.00000000e+00],\n",
       "       [ 1.35014513e+00,  1.24533870e+00,  1.30613225e+00,\n",
       "         1.16880754e+00,  1.01037056e+00,  4.61748613e-01,\n",
       "        -8.87010833e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.35014513e+00,  6.47576126e-01,  9.57830320e-01,\n",
       "         5.54543724e-01,  1.10441183e+00,  4.80988769e-01,\n",
       "        -8.76726756e-01, -5.00000000e-01,  0.00000000e+00],\n",
       "       [ 1.02393557e+00,  3.48694837e-01,  9.57830320e-01,\n",
       "         8.61675633e-01,  1.25965355e+00,  5.00000000e-01,\n",
       "        -8.66025404e-01,  2.50000000e-01,  1.00000000e+00],\n",
       "       [ 1.35014513e+00,  1.54421999e+00,  1.65443419e+00,\n",
       "         1.78307136e+00,  1.61058062e+00,  5.18773258e-01,\n",
       "        -8.54911871e-01,  7.50000000e-01,  1.00000000e+00],\n",
       "       [ 1.67635469e+00,  1.24533870e+00,  1.65443419e+00,\n",
       "         1.47593945e+00,  1.85792930e+00,  5.37299608e-01,\n",
       "        -8.43391446e-01, -2.50000000e-01,  1.00000000e+00],\n",
       "       [ 2.00256425e+00,  2.44086386e+00,  2.00273612e+00,\n",
       "         1.47593945e+00,  2.08172097e+00,  5.55570233e-01,\n",
       "        -8.31469612e-01,  0.00000000e+00,  1.00000000e+00],\n",
       "       [-1.58574092e+00, -2.49067741e-01, -1.82858516e+00,\n",
       "         2.47411815e-01,  2.00357211e+00, -9.15311479e-01,\n",
       "        -4.02746690e-01, -1.00000000e+00,  0.00000000e+00],\n",
       "       [ 3.71516444e-01,  1.24533870e+00,  6.09528385e-01,\n",
       "         1.78307136e+00,  2.28364979e+00, -9.06307787e-01,\n",
       "        -4.22618262e-01,  1.25000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split into train and test sets\n",
    "# Train Set will be from 8/1/17 through 12/31/17, Test Set 1/1/17 - 1/25/17\n",
    "p = 8547 #Manual split for now\n",
    "p = int(len(X) * 0.9)\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "X_train = X[0:p]\n",
    "Y_train = Y[0:p]\n",
    "X_test = X[p:]\n",
    "Y_test = Y[p:]\n",
    "\n",
    "#We may want to shuffle the training data -- will look into this later\n",
    "def shuffle_in_unison(a, b):\n",
    "    # courtsey http://stackoverflow.com/users/190280/josh-bleecher-snyder\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
    "\n",
    "# Not sure why this is needed, but we apply it anyway\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1], EMB_SIZE))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1], EMB_SIZE))\n",
    "X_train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_53 (TimeDis (None, None, 270, 480, 32 4736      \n",
      "=================================================================\n",
      "Total params: 4,736\n",
      "Trainable params: 4,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        Conv2D(32, (7, 7), padding='same', strides=2),\n",
    "        input_shape=(None, 540, 960, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_61 (TimeDis (None, None, 36, 32)      160       \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, None, 36, 32)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, None, 36, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_64 (TimeDis (None, None, 36, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, None, 36, 64)      8256      \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, None, 36, 64)      256       \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, None, 36, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, None, 36, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, None, 2304)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_70 (TimeDis (None, None, 32)          73760     \n",
      "_________________________________________________________________\n",
      "time_distributed_71 (TimeDis (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "time_distributed_72 (TimeDis (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 107,650\n",
      "Trainable params: 107,394\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Convolution1D(filters=32,\n",
    "                                        kernel_size=2,\n",
    "                                        padding='same'),\n",
    "                          input_shape = (None, WINDOW, EMB_SIZE)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(LeakyReLU()))\n",
    "model.add(TimeDistributed(Dropout(0.75)))\n",
    "\n",
    "model.add(TimeDistributed(Convolution1D(filters=64,\n",
    "                        kernel_size=4,\n",
    "                        padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(LeakyReLU()))\n",
    "model.add(TimeDistributed(Dropout(0.75)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(TimeDistributed((Dense(32))))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(LeakyReLU()))\n",
    "\n",
    "model.add(LSTM(64, recurrent_dropout=0.75))\n",
    "#model.add(Dropout(0))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 36, 32)            608       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 36, 32)            128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 36, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 36, 64)            8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 36, 64)            256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 36, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                73760     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 83,074\n",
      "Trainable params: 82,882\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (WINDOW, EMB_SIZE),\n",
    "                        filters=32,\n",
    "                        kernel_size=2,\n",
    "                        padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "#model.add(MaxPooling1D(strides=1))\n",
    "#model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Convolution1D(filters=64,\n",
    "                        kernel_size=4,\n",
    "                        padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "#model.add(MaxPooling1D(strides=1))\n",
    "#model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 19,074\n",
      "Trainable params: 19,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=False,\n",
    "               input_shape=(WINDOW, EMB_SIZE),\n",
    "              dropout=0,\n",
    "              recurrent_dropout=0.75))  # returns a sequence of vectors of dimension 32\n",
    "#model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "#model.add(LSTM(32, recurrent_dropout=0.75))  # return a single vector of dimension 32\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cnn_input (InputLayer)          (None, 36, 9)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 36, 32)       608         cnn_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 36, 32)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 36, 64)       8256        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 36, 64)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2304)         0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           73760       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 32)           5376        cnn_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64)           0           dense_13[0][0]                   \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           2080        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            66          dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 90,146\n",
      "Trainable params: 90,146\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_input = Input(shape=(WINDOW, EMB_SIZE), dtype='float32', name='cnn_input')\n",
    "x = Convolution1D(filters=32,\n",
    "                  kernel_size=2,\n",
    "                  padding='same',\n",
    "                  activation='relu')(cnn_input)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Convolution1D(filters=64,\n",
    "                  kernel_size=4,\n",
    "                  padding='same',\n",
    "                  activation='relu')(x)\n",
    "x = Dropout(0.75)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "x1 = LSTM(32, recurrent_dropout=0.75)(cnn_input)\n",
    "\n",
    "merged = concatenate([x, x1], axis=-1)\n",
    "x2 = Dense(32, activation='relu')(merged)\n",
    "\n",
    "output = Dense(2, activation='softmax', name='output')(x2)\n",
    "model = Model(inputs=[cnn_input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9571 samples, validate on 1064 samples\n",
      "Epoch 1/200\n",
      "9571/9571 [==============================] - 6s 583us/step - loss: 0.7244 - acc: 0.5266 - precision: 0.4488 - precision_1: 0.4112 - val_loss: 0.6899 - val_acc: 0.5498 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68992, saving model to model.hdf5\n",
      "Epoch 2/200\n",
      "9571/9571 [==============================] - 4s 425us/step - loss: 0.6964 - acc: 0.5331 - precision: 0.4517 - precision_1: 0.3280 - val_loss: 0.6907 - val_acc: 0.5498 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68992\n",
      "Epoch 3/200\n",
      "9571/9571 [==============================] - 4s 433us/step - loss: 0.6936 - acc: 0.5375 - precision: 0.3605 - precision_1: 0.1337 - val_loss: 0.6900 - val_acc: 0.5498 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68992\n",
      "Epoch 4/200\n",
      "9571/9571 [==============================] - 4s 432us/step - loss: 0.6913 - acc: 0.5456 - precision: 0.1878 - precision_1: 0.0468 - val_loss: 0.6905 - val_acc: 0.5470 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.68992\n",
      "Epoch 5/200\n",
      "9571/9571 [==============================] - 4s 435us/step - loss: 0.6907 - acc: 0.5462 - precision: 0.1705 - precision_1: 0.0201 - val_loss: 0.6912 - val_acc: 0.5348 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.68992\n",
      "Epoch 6/200\n",
      "9571/9571 [==============================] - 4s 432us/step - loss: 0.6899 - acc: 0.5424 - precision: 0.1020 - precision_1: 0.0134 - val_loss: 0.6899 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.68992 to 0.68989, saving model to model.hdf5\n",
      "Epoch 7/200\n",
      "9571/9571 [==============================] - 4s 434us/step - loss: 0.6895 - acc: 0.5451 - precision: 0.1036 - precision_1: 0.0267 - val_loss: 0.6894 - val_acc: 0.5451 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68989 to 0.68941, saving model to model.hdf5\n",
      "Epoch 8/200\n",
      "9571/9571 [==============================] - 4s 435us/step - loss: 0.6898 - acc: 0.5483 - precision: 0.0502 - precision_1: 0.0100 - val_loss: 0.6899 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.68941\n",
      "Epoch 9/200\n",
      "9571/9571 [==============================] - 4s 437us/step - loss: 0.6884 - acc: 0.5517 - precision: 0.0201 - precision_1: 0.0067 - val_loss: 0.6898 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.68941\n",
      "Epoch 10/200\n",
      "9571/9571 [==============================] - 4s 430us/step - loss: 0.6891 - acc: 0.5516 - precision: 0.0267 - precision_1: 0.0134 - val_loss: 0.6895 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.68941\n",
      "Epoch 11/200\n",
      "9571/9571 [==============================] - 4s 429us/step - loss: 0.6876 - acc: 0.5510 - precision: 0.0201 - precision_1: 0.0000e+00 - val_loss: 0.6897 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.68941\n",
      "Epoch 12/200\n",
      "9571/9571 [==============================] - 4s 441us/step - loss: 0.6880 - acc: 0.5525 - precision: 0.0267 - precision_1: 0.0000e+00 - val_loss: 0.6899 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.68941\n",
      "Epoch 13/200\n",
      "9571/9571 [==============================] - 4s 435us/step - loss: 0.6878 - acc: 0.5525 - precision: 0.0134 - precision_1: 0.0000e+00 - val_loss: 0.6899 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.68941\n",
      "Epoch 14/200\n",
      "9571/9571 [==============================] - 4s 436us/step - loss: 0.6875 - acc: 0.5525 - precision: 0.0067 - precision_1: 0.0067 - val_loss: 0.6899 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.68941\n",
      "Epoch 15/200\n",
      "9571/9571 [==============================] - 4s 433us/step - loss: 0.6875 - acc: 0.5540 - precision: 0.0067 - precision_1: 0.0067 - val_loss: 0.6900 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.68941\n",
      "Epoch 16/200\n",
      "9571/9571 [==============================] - 4s 436us/step - loss: 0.6872 - acc: 0.5540 - precision: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.6901 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.68941\n",
      "Epoch 17/200\n",
      "9571/9571 [==============================] - 4s 438us/step - loss: 0.6869 - acc: 0.5537 - precision: 0.0067 - precision_1: 0.0000e+00 - val_loss: 0.6903 - val_acc: 0.5451 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.68941\n",
      "Epoch 18/200\n",
      "9571/9571 [==============================] - 4s 434us/step - loss: 0.6870 - acc: 0.5549 - precision: 0.0134 - precision_1: 0.0067 - val_loss: 0.6898 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.68941\n",
      "Epoch 19/200\n",
      "9571/9571 [==============================] - 4s 440us/step - loss: 0.6871 - acc: 0.5546 - precision: 0.0067 - precision_1: 0.0000e+00 - val_loss: 0.6900 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.68941\n",
      "Epoch 20/200\n",
      "9571/9571 [==============================] - 4s 444us/step - loss: 0.6874 - acc: 0.5534 - precision: 0.0067 - precision_1: 0.0000e+00 - val_loss: 0.6899 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.68941\n",
      "Epoch 21/200\n",
      "9571/9571 [==============================] - 4s 440us/step - loss: 0.6869 - acc: 0.5533 - precision: 0.0067 - precision_1: 0.0000e+00 - val_loss: 0.6895 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.68941\n",
      "Epoch 22/200\n",
      "9571/9571 [==============================] - 4s 437us/step - loss: 0.6868 - acc: 0.5531 - precision: 0.0134 - precision_1: 0.0000e+00 - val_loss: 0.6896 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68941\n",
      "Epoch 23/200\n",
      "9571/9571 [==============================] - 4s 439us/step - loss: 0.6869 - acc: 0.5548 - precision: 0.0234 - precision_1: 0.0067 - val_loss: 0.6891 - val_acc: 0.5451 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.68941 to 0.68907, saving model to model.hdf5\n",
      "Epoch 24/200\n",
      "9571/9571 [==============================] - 4s 442us/step - loss: 0.6871 - acc: 0.5532 - precision: 0.0267 - precision_1: 0.0134 - val_loss: 0.6893 - val_acc: 0.5442 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.68907\n",
      "Epoch 25/200\n",
      "9571/9571 [==============================] - 4s 442us/step - loss: 0.6868 - acc: 0.5540 - precision: 0.0334 - precision_1: 0.0067 - val_loss: 0.6890 - val_acc: 0.5479 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.68907 to 0.68896, saving model to model.hdf5\n",
      "Epoch 26/200\n",
      "9571/9571 [==============================] - 4s 439us/step - loss: 0.6862 - acc: 0.5540 - precision: 0.0635 - precision_1: 0.0267 - val_loss: 0.6887 - val_acc: 0.5498 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.68896 to 0.68870, saving model to model.hdf5\n",
      "Epoch 27/200\n",
      "9571/9571 [==============================] - 4s 439us/step - loss: 0.6866 - acc: 0.5535 - precision: 0.0267 - precision_1: 0.0067 - val_loss: 0.6890 - val_acc: 0.5489 - val_precision: 0.0602 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.68870\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9571/9571 [==============================] - 4s 414us/step - loss: 0.6865 - acc: 0.5543 - precision: 0.0201 - precision_1: 0.0134 - val_loss: 0.6893 - val_acc: 0.5498 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68870\n",
      "Epoch 29/200\n",
      "9571/9571 [==============================] - 4s 408us/step - loss: 0.6872 - acc: 0.5547 - precision: 0.0301 - precision_1: 0.0000e+00 - val_loss: 0.6890 - val_acc: 0.5498 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.68870\n",
      "Epoch 30/200\n",
      "9571/9571 [==============================] - 4s 409us/step - loss: 0.6865 - acc: 0.5548 - precision: 0.0134 - precision_1: 0.0067 - val_loss: 0.6892 - val_acc: 0.5470 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.68870\n",
      "Epoch 31/200\n",
      "9571/9571 [==============================] - 4s 414us/step - loss: 0.6857 - acc: 0.5549 - precision: 0.0067 - precision_1: 0.0000e+00 - val_loss: 0.6891 - val_acc: 0.5479 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002700000128243119.\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.68870\n",
      "Epoch 32/200\n",
      "9571/9571 [==============================] - 4s 417us/step - loss: 0.6860 - acc: 0.5539 - precision: 0.0267 - precision_1: 0.0267 - val_loss: 0.6889 - val_acc: 0.5479 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68870\n",
      "Epoch 33/200\n",
      "9571/9571 [==============================] - 4s 412us/step - loss: 0.6867 - acc: 0.5545 - precision: 0.0134 - precision_1: 0.0000e+00 - val_loss: 0.6894 - val_acc: 0.5461 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.68870\n",
      "Epoch 34/200\n",
      "9571/9571 [==============================] - 4s 422us/step - loss: 0.6862 - acc: 0.5546 - precision: 0.0201 - precision_1: 0.0067 - val_loss: 0.6892 - val_acc: 0.5489 - val_precision: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.68870\n",
      "Epoch 35/200\n",
      "9571/9571 [==============================] - 4s 407us/step - loss: 0.6853 - acc: 0.5534 - precision: 0.0134 - precision_1: 0.0067 - val_loss: 0.6894 - val_acc: 0.5498 - val_precision: 0.0401 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.68870\n",
      "Epoch 36/200\n",
      "9571/9571 [==============================] - 4s 418us/step - loss: 0.6863 - acc: 0.5533 - precision: 0.0067 - precision_1: 0.0000e+00 - val_loss: 0.6895 - val_acc: 0.5489 - val_precision: 0.0401 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.68870\n",
      "Epoch 37/200\n",
      "9571/9571 [==============================] - 4s 418us/step - loss: 0.6854 - acc: 0.5540 - precision: 0.0134 - precision_1: 0.0067 - val_loss: 0.6891 - val_acc: 0.5517 - val_precision: 0.0401 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.68870\n",
      "Epoch 38/200\n",
      "9571/9571 [==============================] - 4s 416us/step - loss: 0.6863 - acc: 0.5540 - precision: 0.0267 - precision_1: 0.0134 - val_loss: 0.6892 - val_acc: 0.5508 - val_precision: 0.0401 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.68870\n",
      "Epoch 39/200\n",
      "9571/9571 [==============================] - 4s 419us/step - loss: 0.6851 - acc: 0.5552 - precision: 0.0301 - precision_1: 0.0067 - val_loss: 0.6887 - val_acc: 0.5508 - val_precision: 0.0401 - val_precision_1: 0.0000e+00\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.68870\n",
      "Epoch 40/200\n",
      "9571/9571 [==============================] - 4s 419us/step - loss: 0.6855 - acc: 0.5570 - precision: 0.0568 - precision_1: 0.0000e+00 - val_loss: 0.6883 - val_acc: 0.5526 - val_precision: 0.1053 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.68870 to 0.68834, saving model to model.hdf5\n",
      "Epoch 41/200\n",
      "9571/9571 [==============================] - 4s 409us/step - loss: 0.6863 - acc: 0.5539 - precision: 0.0502 - precision_1: 0.0067 - val_loss: 0.6889 - val_acc: 0.5479 - val_precision: 0.0401 - val_precision_1: 0.0301\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.68834\n",
      "Epoch 42/200\n",
      "9571/9571 [==============================] - 4s 415us/step - loss: 0.6849 - acc: 0.5552 - precision: 0.0535 - precision_1: 0.0067 - val_loss: 0.6880 - val_acc: 0.5536 - val_precision: 0.1654 - val_precision_1: 0.0301\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.68834 to 0.68801, saving model to model.hdf5\n",
      "Epoch 43/200\n",
      "9571/9571 [==============================] - 4s 415us/step - loss: 0.6860 - acc: 0.5548 - precision: 0.0702 - precision_1: 0.0134 - val_loss: 0.6887 - val_acc: 0.5489 - val_precision: 0.0401 - val_precision_1: 0.0301\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.68801\n",
      "Epoch 44/200\n",
      "9571/9571 [==============================] - 4s 414us/step - loss: 0.6850 - acc: 0.5548 - precision: 0.0401 - precision_1: 0.0067 - val_loss: 0.6891 - val_acc: 0.5536 - val_precision: 0.0451 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.68801\n",
      "Epoch 45/200\n",
      "9571/9571 [==============================] - 4s 415us/step - loss: 0.6853 - acc: 0.5556 - precision: 0.0936 - precision_1: 0.0334 - val_loss: 0.6883 - val_acc: 0.5489 - val_precision: 0.0401 - val_precision_1: 0.0301\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.68801\n",
      "Epoch 46/200\n",
      "9571/9571 [==============================] - 4s 415us/step - loss: 0.6856 - acc: 0.5524 - precision: 0.0970 - precision_1: 0.0401 - val_loss: 0.6882 - val_acc: 0.5498 - val_precision: 0.0451 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.68801\n",
      "Epoch 47/200\n",
      "9571/9571 [==============================] - 4s 418us/step - loss: 0.6844 - acc: 0.5564 - precision: 0.1036 - precision_1: 0.0267 - val_loss: 0.6891 - val_acc: 0.5498 - val_precision: 0.2256 - val_precision_1: 0.0301\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.68801\n",
      "Epoch 48/200\n",
      "9571/9571 [==============================] - 4s 412us/step - loss: 0.6848 - acc: 0.5567 - precision: 0.1103 - precision_1: 0.0334 - val_loss: 0.6882 - val_acc: 0.5555 - val_precision: 0.1053 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.68801\n",
      "Epoch 49/200\n",
      "9571/9571 [==============================] - 4s 421us/step - loss: 0.6846 - acc: 0.5589 - precision: 0.1371 - precision_1: 0.0502 - val_loss: 0.6877 - val_acc: 0.5489 - val_precision: 0.2767 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.68801 to 0.68771, saving model to model.hdf5\n",
      "Epoch 50/200\n",
      "9571/9571 [==============================] - 4s 453us/step - loss: 0.6855 - acc: 0.5524 - precision: 0.0970 - precision_1: 0.0334 - val_loss: 0.6878 - val_acc: 0.5508 - val_precision: 0.1654 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.68771\n",
      "Epoch 51/200\n",
      "9571/9571 [==============================] - 4s 458us/step - loss: 0.6843 - acc: 0.5545 - precision: 0.1271 - precision_1: 0.0602 - val_loss: 0.6879 - val_acc: 0.5508 - val_precision: 0.2857 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.68771\n",
      "Epoch 52/200\n",
      "9571/9571 [==============================] - 4s 457us/step - loss: 0.6846 - acc: 0.5576 - precision: 0.1460 - precision_1: 0.0535 - val_loss: 0.6875 - val_acc: 0.5508 - val_precision: 0.2857 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.68771 to 0.68754, saving model to model.hdf5\n",
      "Epoch 53/200\n",
      "9571/9571 [==============================] - 4s 453us/step - loss: 0.6842 - acc: 0.5558 - precision: 0.1337 - precision_1: 0.0669 - val_loss: 0.6873 - val_acc: 0.5536 - val_precision: 0.3368 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.68754 to 0.68730, saving model to model.hdf5\n",
      "Epoch 54/200\n",
      "9571/9571 [==============================] - 4s 462us/step - loss: 0.6841 - acc: 0.5568 - precision: 0.1839 - precision_1: 0.0669 - val_loss: 0.6881 - val_acc: 0.5498 - val_precision: 0.3158 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.68730\n",
      "Epoch 55/200\n",
      "9571/9571 [==============================] - 4s 453us/step - loss: 0.6835 - acc: 0.5564 - precision: 0.1891 - precision_1: 0.0602 - val_loss: 0.6873 - val_acc: 0.5573 - val_precision: 0.2957 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.68730 to 0.68728, saving model to model.hdf5\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9571/9571 [==============================] - 4s 415us/step - loss: 0.6849 - acc: 0.5564 - precision: 0.2374 - precision_1: 0.0468 - val_loss: 0.6871 - val_acc: 0.5583 - val_precision: 0.3158 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.68728 to 0.68715, saving model to model.hdf5\n",
      "Epoch 57/200\n",
      "9571/9571 [==============================] - 4s 430us/step - loss: 0.6846 - acc: 0.5533 - precision: 0.2233 - precision_1: 0.0802 - val_loss: 0.6877 - val_acc: 0.5470 - val_precision: 0.3158 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.68715\n",
      "Epoch 58/200\n",
      "9571/9571 [==============================] - 4s 427us/step - loss: 0.6833 - acc: 0.5591 - precision: 0.2374 - precision_1: 0.0602 - val_loss: 0.6871 - val_acc: 0.5526 - val_precision: 0.2256 - val_precision_1: 0.0301\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.68715 to 0.68707, saving model to model.hdf5\n",
      "Epoch 59/200\n",
      "9571/9571 [==============================] - 4s 425us/step - loss: 0.6840 - acc: 0.5567 - precision: 0.2574 - precision_1: 0.0936 - val_loss: 0.6867 - val_acc: 0.5583 - val_precision: 0.3459 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.68707 to 0.68669, saving model to model.hdf5\n",
      "Epoch 60/200\n",
      "9571/9571 [==============================] - 4s 424us/step - loss: 0.6839 - acc: 0.5556 - precision: 0.1605 - precision_1: 0.0468 - val_loss: 0.6873 - val_acc: 0.5470 - val_precision: 0.3459 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.68669\n",
      "Epoch 61/200\n",
      "9571/9571 [==============================] - 4s 430us/step - loss: 0.6843 - acc: 0.5551 - precision: 0.2039 - precision_1: 0.0769 - val_loss: 0.6876 - val_acc: 0.5498 - val_precision: 0.1654 - val_precision_1: 0.0401\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.68669\n",
      "Epoch 62/200\n",
      "9571/9571 [==============================] - 4s 426us/step - loss: 0.6836 - acc: 0.5565 - precision: 0.1630 - precision_1: 0.0802 - val_loss: 0.6875 - val_acc: 0.5479 - val_precision: 0.3459 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.68669\n",
      "Epoch 63/200\n",
      "9571/9571 [==============================] - 4s 438us/step - loss: 0.6831 - acc: 0.5551 - precision: 0.1594 - precision_1: 0.0669 - val_loss: 0.6869 - val_acc: 0.5461 - val_precision: 0.3258 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.68669\n",
      "Epoch 64/200\n",
      "9571/9571 [==============================] - 4s 451us/step - loss: 0.6831 - acc: 0.5564 - precision: 0.2335 - precision_1: 0.0936 - val_loss: 0.6864 - val_acc: 0.5526 - val_precision: 0.3258 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.68669 to 0.68638, saving model to model.hdf5\n",
      "Epoch 65/200\n",
      "9571/9571 [==============================] - 4s 426us/step - loss: 0.6834 - acc: 0.5586 - precision: 0.1904 - precision_1: 0.0702 - val_loss: 0.6873 - val_acc: 0.5395 - val_precision: 0.4060 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.68638\n",
      "Epoch 66/200\n",
      "9571/9571 [==============================] - 4s 422us/step - loss: 0.6822 - acc: 0.5567 - precision: 0.2856 - precision_1: 0.1337 - val_loss: 0.6864 - val_acc: 0.5583 - val_precision: 0.3258 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.68638\n",
      "Epoch 67/200\n",
      "9571/9571 [==============================] - 4s 429us/step - loss: 0.6824 - acc: 0.5562 - precision: 0.3584 - precision_1: 0.1672 - val_loss: 0.6867 - val_acc: 0.5536 - val_precision: 0.2857 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.68638\n",
      "Epoch 68/200\n",
      "9571/9571 [==============================] - 4s 423us/step - loss: 0.6833 - acc: 0.5530 - precision: 0.2173 - precision_1: 0.1003 - val_loss: 0.6866 - val_acc: 0.5545 - val_precision: 0.3459 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.68638\n",
      "Epoch 69/200\n",
      "9571/9571 [==============================] - 4s 424us/step - loss: 0.6833 - acc: 0.5551 - precision: 0.2145 - precision_1: 0.1204 - val_loss: 0.6867 - val_acc: 0.5517 - val_precision: 0.3258 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.68638\n",
      "Epoch 70/200\n",
      "9571/9571 [==============================] - 4s 429us/step - loss: 0.6831 - acc: 0.5580 - precision: 0.2842 - precision_1: 0.0836 - val_loss: 0.6867 - val_acc: 0.5545 - val_precision: 0.3258 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.68638\n",
      "Epoch 71/200\n",
      "9571/9571 [==============================] - 4s 422us/step - loss: 0.6819 - acc: 0.5608 - precision: 0.2574 - precision_1: 0.0936 - val_loss: 0.6863 - val_acc: 0.5583 - val_precision: 0.2907 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.68638 to 0.68625, saving model to model.hdf5\n",
      "Epoch 72/200\n",
      "9571/9571 [==============================] - 4s 428us/step - loss: 0.6811 - acc: 0.5608 - precision: 0.3738 - precision_1: 0.1404 - val_loss: 0.6862 - val_acc: 0.5573 - val_precision: 0.4170 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.68625 to 0.68620, saving model to model.hdf5\n",
      "Epoch 73/200\n",
      "9571/9571 [==============================] - 4s 430us/step - loss: 0.6830 - acc: 0.5575 - precision: 0.2485 - precision_1: 0.0836 - val_loss: 0.6870 - val_acc: 0.5470 - val_precision: 0.3308 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.68620\n",
      "Epoch 74/200\n",
      "9571/9571 [==============================] - 4s 431us/step - loss: 0.6831 - acc: 0.5603 - precision: 0.2675 - precision_1: 0.0970 - val_loss: 0.6871 - val_acc: 0.5555 - val_precision: 0.3308 - val_precision_1: 0.0451\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.68620\n",
      "Epoch 75/200\n",
      "9571/9571 [==============================] - 4s 434us/step - loss: 0.6821 - acc: 0.5572 - precision: 0.2541 - precision_1: 0.1282 - val_loss: 0.6875 - val_acc: 0.5564 - val_precision: 0.3068 - val_precision_1: 0.1654\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.68620\n",
      "Epoch 76/200\n",
      "9571/9571 [==============================] - 4s 430us/step - loss: 0.6833 - acc: 0.5600 - precision: 0.2976 - precision_1: 0.1326 - val_loss: 0.6878 - val_acc: 0.5479 - val_precision: 0.3118 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.68620\n",
      "Epoch 77/200\n",
      "9571/9571 [==============================] - 4s 431us/step - loss: 0.6816 - acc: 0.5598 - precision: 0.3265 - precision_1: 0.1371 - val_loss: 0.6877 - val_acc: 0.5526 - val_precision: 0.2917 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.68620\n",
      "Epoch 78/200\n",
      "9571/9571 [==============================] - 4s 445us/step - loss: 0.6824 - acc: 0.5586 - precision: 0.3054 - precision_1: 0.1237 - val_loss: 0.6877 - val_acc: 0.5479 - val_precision: 0.3268 - val_precision_1: 0.1654\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.68620\n",
      "Epoch 79/200\n",
      "9571/9571 [==============================] - 4s 449us/step - loss: 0.6819 - acc: 0.5587 - precision: 0.3979 - precision_1: 0.1872 - val_loss: 0.6875 - val_acc: 0.5526 - val_precision: 0.3368 - val_precision_1: 0.1053\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.68620\n",
      "Epoch 80/200\n",
      "9571/9571 [==============================] - 4s 419us/step - loss: 0.6816 - acc: 0.5564 - precision: 0.3210 - precision_1: 0.1237 - val_loss: 0.6874 - val_acc: 0.5461 - val_precision: 0.3719 - val_precision_1: 0.1654\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.68620\n",
      "Epoch 81/200\n",
      "9571/9571 [==============================] - ETA: 0s - loss: 0.6816 - acc: 0.5627 - precision: 0.2883 - precision_1: 0.131 - 4s 394us/step - loss: 0.6813 - acc: 0.5633 - precision: 0.2853 - precision_1: 0.1304 - val_loss: 0.6872 - val_acc: 0.5508 - val_precision: 0.3318 - val_precision_1: 0.1654\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.68620\n",
      "Epoch 82/200\n",
      "9571/9571 [==============================] - 4s 394us/step - loss: 0.6813 - acc: 0.5630 - precision: 0.3973 - precision_1: 0.2039 - val_loss: 0.6870 - val_acc: 0.5555 - val_precision: 0.3794 - val_precision_1: 0.2256\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.68620\n",
      "Epoch 83/200\n",
      "9571/9571 [==============================] - 4s 392us/step - loss: 0.6823 - acc: 0.5594 - precision: 0.3288 - precision_1: 0.1638 - val_loss: 0.6874 - val_acc: 0.5508 - val_precision: 0.3694 - val_precision_1: 0.2256\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.68620\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9571/9571 [==============================] - 4s 385us/step - loss: 0.6816 - acc: 0.5593 - precision: 0.3208 - precision_1: 0.1407 - val_loss: 0.6873 - val_acc: 0.5470 - val_precision: 0.3419 - val_precision_1: 0.2256\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.68620\n",
      "Epoch 85/200\n",
      "9571/9571 [==============================] - 4s 383us/step - loss: 0.6819 - acc: 0.5589 - precision: 0.3358 - precision_1: 0.1772 - val_loss: 0.6866 - val_acc: 0.5536 - val_precision: 0.4246 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.68620\n",
      "Epoch 86/200\n",
      "9571/9571 [==============================] - 4s 385us/step - loss: 0.6802 - acc: 0.5575 - precision: 0.4272 - precision_1: 0.2039 - val_loss: 0.6870 - val_acc: 0.5461 - val_precision: 0.4246 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.00024300001678057015.\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.68620\n",
      "Epoch 87/200\n",
      "9571/9571 [==============================] - 4s 385us/step - loss: 0.6804 - acc: 0.5610 - precision: 0.4079 - precision_1: 0.2441 - val_loss: 0.6868 - val_acc: 0.5461 - val_precision: 0.3644 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.68620\n",
      "Epoch 88/200\n",
      "9571/9571 [==============================] - 4s 383us/step - loss: 0.6817 - acc: 0.5636 - precision: 0.4624 - precision_1: 0.2407 - val_loss: 0.6874 - val_acc: 0.5536 - val_precision: 0.3118 - val_precision_1: 0.2256\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.68620\n",
      "Epoch 89/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6800 - acc: 0.5577 - precision: 0.4348 - precision_1: 0.2235 - val_loss: 0.6875 - val_acc: 0.5470 - val_precision: 0.4697 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.68620\n",
      "Epoch 90/200\n",
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6819 - acc: 0.5537 - precision: 0.4139 - precision_1: 0.2073 - val_loss: 0.6876 - val_acc: 0.5489 - val_precision: 0.4095 - val_precision_1: 0.2165\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.68620\n",
      "Epoch 91/200\n",
      "9571/9571 [==============================] - 4s 383us/step - loss: 0.6806 - acc: 0.5593 - precision: 0.4631 - precision_1: 0.2574 - val_loss: 0.6878 - val_acc: 0.5432 - val_precision: 0.4446 - val_precision_1: 0.1654\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.68620\n",
      "Epoch 92/200\n",
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6818 - acc: 0.5587 - precision: 0.3882 - precision_1: 0.2073 - val_loss: 0.6876 - val_acc: 0.5517 - val_precision: 0.3444 - val_precision_1: 0.0962\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.68620\n",
      "Epoch 93/200\n",
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6801 - acc: 0.5623 - precision: 0.4327 - precision_1: 0.1883 - val_loss: 0.6875 - val_acc: 0.5489 - val_precision: 0.3895 - val_precision_1: 0.1654\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.68620\n",
      "Epoch 94/200\n",
      "9571/9571 [==============================] - 4s 390us/step - loss: 0.6786 - acc: 0.5609 - precision: 0.4296 - precision_1: 0.1906 - val_loss: 0.6875 - val_acc: 0.5498 - val_precision: 0.4847 - val_precision_1: 0.2165\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.68620\n",
      "Epoch 95/200\n",
      "9571/9571 [==============================] - 4s 393us/step - loss: 0.6801 - acc: 0.5575 - precision: 0.4628 - precision_1: 0.2407 - val_loss: 0.6874 - val_acc: 0.5526 - val_precision: 0.5409 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.68620\n",
      "Epoch 96/200\n",
      "9571/9571 [==============================] - 4s 390us/step - loss: 0.6802 - acc: 0.5654 - precision: 0.4604 - precision_1: 0.1984 - val_loss: 0.6873 - val_acc: 0.5461 - val_precision: 0.4596 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.68620\n",
      "Epoch 97/200\n",
      "9571/9571 [==============================] - 4s 391us/step - loss: 0.6805 - acc: 0.5609 - precision: 0.4391 - precision_1: 0.2251 - val_loss: 0.6878 - val_acc: 0.5461 - val_precision: 0.3519 - val_precision_1: 0.2165\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.68620\n",
      "Epoch 98/200\n",
      "9571/9571 [==============================] - 4s 389us/step - loss: 0.6800 - acc: 0.5638 - precision: 0.4441 - precision_1: 0.2140 - val_loss: 0.6879 - val_acc: 0.5442 - val_precision: 0.4311 - val_precision_1: 0.2165\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.68620\n",
      "Epoch 99/200\n",
      "9571/9571 [==============================] - 4s 384us/step - loss: 0.6797 - acc: 0.5648 - precision: 0.4508 - precision_1: 0.2039 - val_loss: 0.6881 - val_acc: 0.5404 - val_precision: 0.3559 - val_precision_1: 0.2165\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.68620\n",
      "Epoch 100/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6801 - acc: 0.5576 - precision: 0.3362 - precision_1: 0.2106 - val_loss: 0.6880 - val_acc: 0.5479 - val_precision: 0.4747 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.68620\n",
      "Epoch 101/200\n",
      "9571/9571 [==============================] - 4s 388us/step - loss: 0.6803 - acc: 0.5610 - precision: 0.4766 - precision_1: 0.2619 - val_loss: 0.6880 - val_acc: 0.5451 - val_precision: 0.4687 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.68620\n",
      "Epoch 102/200\n",
      "9571/9571 [==============================] - 4s 384us/step - loss: 0.6781 - acc: 0.5652 - precision: 0.4670 - precision_1: 0.2474 - val_loss: 0.6883 - val_acc: 0.5517 - val_precision: 0.5255 - val_precision_1: 0.3368\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.68620\n",
      "Epoch 103/200\n",
      "9571/9571 [==============================] - 4s 383us/step - loss: 0.6785 - acc: 0.5664 - precision: 0.4695 - precision_1: 0.2608 - val_loss: 0.6881 - val_acc: 0.5479 - val_precision: 0.4386 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.68620\n",
      "Epoch 104/200\n",
      "9571/9571 [==============================] - 4s 385us/step - loss: 0.6808 - acc: 0.5628 - precision: 0.4668 - precision_1: 0.2820 - val_loss: 0.6881 - val_acc: 0.5451 - val_precision: 0.4085 - val_precision_1: 0.2165\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.68620\n",
      "Epoch 105/200\n",
      "9571/9571 [==============================] - 4s 383us/step - loss: 0.6782 - acc: 0.5654 - precision: 0.4872 - precision_1: 0.2834 - val_loss: 0.6878 - val_acc: 0.5508 - val_precision: 0.3935 - val_precision_1: 0.3368\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.68620\n",
      "Epoch 106/200\n",
      "9571/9571 [==============================] - 4s 390us/step - loss: 0.6803 - acc: 0.5601 - precision: 0.4928 - precision_1: 0.2574 - val_loss: 0.6876 - val_acc: 0.5526 - val_precision: 0.4657 - val_precision_1: 0.2466\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.68620\n",
      "Epoch 107/200\n",
      "9571/9571 [==============================] - 4s 388us/step - loss: 0.6787 - acc: 0.5682 - precision: 0.5471 - precision_1: 0.2513 - val_loss: 0.6876 - val_acc: 0.5489 - val_precision: 0.3935 - val_precision_1: 0.2767\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.68620\n",
      "Epoch 108/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6787 - acc: 0.5690 - precision: 0.4941 - precision_1: 0.2367 - val_loss: 0.6881 - val_acc: 0.5479 - val_precision: 0.3835 - val_precision_1: 0.3368\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.68620\n",
      "Epoch 109/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6786 - acc: 0.5634 - precision: 0.4417 - precision_1: 0.2207 - val_loss: 0.6881 - val_acc: 0.5508 - val_precision: 0.3885 - val_precision_1: 0.3368\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.68620\n",
      "Epoch 110/200\n",
      "9571/9571 [==============================] - 4s 389us/step - loss: 0.6780 - acc: 0.5643 - precision: 0.5206 - precision_1: 0.2563 - val_loss: 0.6886 - val_acc: 0.5461 - val_precision: 0.4797 - val_precision_1: 0.3168\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.68620\n",
      "Epoch 111/200\n",
      "9571/9571 [==============================] - 4s 388us/step - loss: 0.6789 - acc: 0.5693 - precision: 0.5118 - precision_1: 0.3232 - val_loss: 0.6882 - val_acc: 0.5432 - val_precision: 0.4657 - val_precision_1: 0.3168\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.68620\n",
      "Epoch 112/200\n",
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6787 - acc: 0.5671 - precision: 0.5255 - precision_1: 0.3009 - val_loss: 0.6881 - val_acc: 0.5451 - val_precision: 0.4556 - val_precision_1: 0.3769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00112: val_loss did not improve from 0.68620\n",
      "Epoch 113/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6769 - acc: 0.5701 - precision: 0.5387 - precision_1: 0.2808 - val_loss: 0.6884 - val_acc: 0.5508 - val_precision: 0.4506 - val_precision_1: 0.3769\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.68620\n",
      "Epoch 114/200\n",
      "9571/9571 [==============================] - 4s 422us/step - loss: 0.6783 - acc: 0.5703 - precision: 0.4809 - precision_1: 0.2882 - val_loss: 0.6884 - val_acc: 0.5479 - val_precision: 0.4095 - val_precision_1: 0.3669\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.68620\n",
      "Epoch 115/200\n",
      "9571/9571 [==============================] - 4s 466us/step - loss: 0.6785 - acc: 0.5685 - precision: 0.5219 - precision_1: 0.3187 - val_loss: 0.6882 - val_acc: 0.5517 - val_precision: 0.4513 - val_precision_1: 0.4145\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.68620\n",
      "Epoch 116/200\n",
      "9571/9571 [==============================] - 4s 432us/step - loss: 0.6782 - acc: 0.5648 - precision: 0.5063 - precision_1: 0.2903 - val_loss: 0.6881 - val_acc: 0.5620 - val_precision: 0.4413 - val_precision_1: 0.3544\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.68620\n",
      "Epoch 117/200\n",
      "9571/9571 [==============================] - 4s 452us/step - loss: 0.6783 - acc: 0.5614 - precision: 0.5415 - precision_1: 0.2909 - val_loss: 0.6883 - val_acc: 0.5526 - val_precision: 0.4987 - val_precision_1: 0.3008\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.68620\n",
      "Epoch 118/200\n",
      "9571/9571 [==============================] - 4s 433us/step - loss: 0.6762 - acc: 0.5705 - precision: 0.5181 - precision_1: 0.3639 - val_loss: 0.6882 - val_acc: 0.5508 - val_precision: 0.4410 - val_precision_1: 0.3243\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.68620\n",
      "Epoch 119/200\n",
      "9571/9571 [==============================] - 4s 437us/step - loss: 0.6762 - acc: 0.5717 - precision: 0.5740 - precision_1: 0.3734 - val_loss: 0.6879 - val_acc: 0.5573 - val_precision: 0.4273 - val_precision_1: 0.3544\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.68620\n",
      "Epoch 120/200\n",
      "9571/9571 [==============================] - 4s 428us/step - loss: 0.6763 - acc: 0.5711 - precision: 0.5641 - precision_1: 0.3391 - val_loss: 0.6881 - val_acc: 0.5489 - val_precision: 0.4100 - val_precision_1: 0.3183\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.68620\n",
      "Epoch 121/200\n",
      "9571/9571 [==============================] - 4s 401us/step - loss: 0.6773 - acc: 0.5680 - precision: 0.5672 - precision_1: 0.3890 - val_loss: 0.6888 - val_acc: 0.5508 - val_precision: 0.4270 - val_precision_1: 0.3183\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.68620\n",
      "Epoch 122/200\n",
      "9571/9571 [==============================] - 4s 397us/step - loss: 0.6788 - acc: 0.5688 - precision: 0.5210 - precision_1: 0.2747 - val_loss: 0.6888 - val_acc: 0.5508 - val_precision: 0.4774 - val_precision_1: 0.3544\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.68620\n",
      "Epoch 123/200\n",
      "9571/9571 [==============================] - 4s 402us/step - loss: 0.6758 - acc: 0.5785 - precision: 0.5804 - precision_1: 0.3555 - val_loss: 0.6883 - val_acc: 0.5508 - val_precision: 0.4967 - val_precision_1: 0.3183\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.68620\n",
      "Epoch 124/200\n",
      "9571/9571 [==============================] - 4s 396us/step - loss: 0.6765 - acc: 0.5694 - precision: 0.5546 - precision_1: 0.3678 - val_loss: 0.6887 - val_acc: 0.5639 - val_precision: 0.5234 - val_precision_1: 0.3008\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.68620\n",
      "Epoch 125/200\n",
      "9571/9571 [==============================] - 4s 393us/step - loss: 0.6758 - acc: 0.5735 - precision: 0.5585 - precision_1: 0.3679 - val_loss: 0.6888 - val_acc: 0.5555 - val_precision: 0.5033 - val_precision_1: 0.2807\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.68620\n",
      "Epoch 126/200\n",
      "9571/9571 [==============================] - 4s 389us/step - loss: 0.6776 - acc: 0.5625 - precision: 0.5352 - precision_1: 0.3569 - val_loss: 0.6888 - val_acc: 0.5555 - val_precision: 0.5471 - val_precision_1: 0.3108\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.68620\n",
      "Epoch 127/200\n",
      "9571/9571 [==============================] - 4s 385us/step - loss: 0.6761 - acc: 0.5675 - precision: 0.5344 - precision_1: 0.3830 - val_loss: 0.6888 - val_acc: 0.5536 - val_precision: 0.4651 - val_precision_1: 0.3784\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.68620\n",
      "Epoch 128/200\n",
      "9571/9571 [==============================] - 4s 404us/step - loss: 0.6772 - acc: 0.5695 - precision: 0.5741 - precision_1: 0.4229 - val_loss: 0.6883 - val_acc: 0.5526 - val_precision: 0.4764 - val_precision_1: 0.4085\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.68620\n",
      "Epoch 129/200\n",
      "9571/9571 [==============================] - 4s 393us/step - loss: 0.6754 - acc: 0.5754 - precision: 0.5611 - precision_1: 0.3756 - val_loss: 0.6882 - val_acc: 0.5508 - val_precision: 0.4714 - val_precision_1: 0.3784\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.68620\n",
      "Epoch 130/200\n",
      "9571/9571 [==============================] - 4s 397us/step - loss: 0.6751 - acc: 0.5662 - precision: 0.5854 - precision_1: 0.4391 - val_loss: 0.6887 - val_acc: 0.5442 - val_precision: 0.4112 - val_precision_1: 0.3784\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.68620\n",
      "Epoch 131/200\n",
      "9571/9571 [==============================] - 4s 403us/step - loss: 0.6746 - acc: 0.5752 - precision: 0.5847 - precision_1: 0.4335 - val_loss: 0.6898 - val_acc: 0.5526 - val_precision: 0.5874 - val_precision_1: 0.3684\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.68620\n",
      "Epoch 132/200\n",
      "9571/9571 [==============================] - 4s 395us/step - loss: 0.6771 - acc: 0.5689 - precision: 0.5452 - precision_1: 0.4012 - val_loss: 0.6894 - val_acc: 0.5602 - val_precision: 0.5563 - val_precision_1: 0.3108\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.68620\n",
      "Epoch 133/200\n",
      "9571/9571 [==============================] - 4s 393us/step - loss: 0.6759 - acc: 0.5750 - precision: 0.6022 - precision_1: 0.3962 - val_loss: 0.6900 - val_acc: 0.5648 - val_precision: 0.5580 - val_precision_1: 0.3308\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.68620\n",
      "Epoch 134/200\n",
      "9571/9571 [==============================] - 4s 461us/step - loss: 0.6738 - acc: 0.5722 - precision: 0.5563 - precision_1: 0.4299 - val_loss: 0.6903 - val_acc: 0.5583 - val_precision: 0.5639 - val_precision_1: 0.3108\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.68620\n",
      "Epoch 135/200\n",
      "9571/9571 [==============================] - 4s 444us/step - loss: 0.6769 - acc: 0.5651 - precision: 0.5884 - precision_1: 0.3675 - val_loss: 0.6903 - val_acc: 0.5526 - val_precision: 0.5242 - val_precision_1: 0.3459\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.68620\n",
      "Epoch 136/200\n",
      "9571/9571 [==============================] - 4s 439us/step - loss: 0.6760 - acc: 0.5690 - precision: 0.5307 - precision_1: 0.4291 - val_loss: 0.6904 - val_acc: 0.5648 - val_precision: 0.5237 - val_precision_1: 0.3108\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.68620\n",
      "Epoch 137/200\n",
      "9571/9571 [==============================] - 4s 442us/step - loss: 0.6769 - acc: 0.5664 - precision: 0.5653 - precision_1: 0.3722 - val_loss: 0.6907 - val_acc: 0.5602 - val_precision: 0.4992 - val_precision_1: 0.2807\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.68620\n",
      "Epoch 138/200\n",
      "9571/9571 [==============================] - 4s 447us/step - loss: 0.6756 - acc: 0.5692 - precision: 0.5546 - precision_1: 0.3611 - val_loss: 0.6898 - val_acc: 0.5517 - val_precision: 0.5465 - val_precision_1: 0.2506\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.68620\n",
      "Epoch 139/200\n",
      "9571/9571 [==============================] - 4s 445us/step - loss: 0.6757 - acc: 0.5689 - precision: 0.5789 - precision_1: 0.4238 - val_loss: 0.6895 - val_acc: 0.5583 - val_precision: 0.6033 - val_precision_1: 0.3509\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.68620\n",
      "Epoch 140/200\n",
      "9571/9571 [==============================] - 4s 445us/step - loss: 0.6767 - acc: 0.5684 - precision: 0.5702 - precision_1: 0.4088 - val_loss: 0.6902 - val_acc: 0.5536 - val_precision: 0.6176 - val_precision_1: 0.2807\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.68620\n",
      "Epoch 141/200\n",
      "9571/9571 [==============================] - 4s 449us/step - loss: 0.6757 - acc: 0.5706 - precision: 0.5764 - precision_1: 0.4249 - val_loss: 0.6910 - val_acc: 0.5648 - val_precision: 0.5849 - val_precision_1: 0.2807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00141: val_loss did not improve from 0.68620\n",
      "Epoch 142/200\n",
      "9571/9571 [==============================] - 4s 416us/step - loss: 0.6766 - acc: 0.5693 - precision: 0.5607 - precision_1: 0.4625 - val_loss: 0.6899 - val_acc: 0.5564 - val_precision: 0.5345 - val_precision_1: 0.2506\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.68620\n",
      "Epoch 143/200\n",
      "9571/9571 [==============================] - 4s 435us/step - loss: 0.6741 - acc: 0.5715 - precision: 0.5807 - precision_1: 0.4129 - val_loss: 0.6906 - val_acc: 0.5517 - val_precision: 0.5722 - val_precision_1: 0.2807\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.68620\n",
      "Epoch 144/200\n",
      "9571/9571 [==============================] - 4s 420us/step - loss: 0.6752 - acc: 0.5754 - precision: 0.5548 - precision_1: 0.4413 - val_loss: 0.6905 - val_acc: 0.5498 - val_precision: 0.5654 - val_precision_1: 0.2907\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.68620\n",
      "Epoch 145/200\n",
      "9571/9571 [==============================] - 4s 420us/step - loss: 0.6748 - acc: 0.5731 - precision: 0.5898 - precision_1: 0.3934 - val_loss: 0.6903 - val_acc: 0.5461 - val_precision: 0.6240 - val_precision_1: 0.2556\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.68620\n",
      "Epoch 146/200\n",
      "9571/9571 [==============================] - 4s 423us/step - loss: 0.6739 - acc: 0.5737 - precision: 0.5679 - precision_1: 0.4545 - val_loss: 0.6906 - val_acc: 0.5479 - val_precision: 0.5777 - val_precision_1: 0.2456\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.68620\n",
      "Epoch 147/200\n",
      "9571/9571 [==============================] - 4s 422us/step - loss: 0.6739 - acc: 0.5787 - precision: 0.5874 - precision_1: 0.3940 - val_loss: 0.6912 - val_acc: 0.5451 - val_precision: 0.5676 - val_precision_1: 0.2456\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.68620\n",
      "Epoch 148/200\n",
      "9571/9571 [==============================] - 4s 436us/step - loss: 0.6729 - acc: 0.5782 - precision: 0.5973 - precision_1: 0.4402 - val_loss: 0.6912 - val_acc: 0.5451 - val_precision: 0.6055 - val_precision_1: 0.2446\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.68620\n",
      "Epoch 149/200\n",
      "9571/9571 [==============================] - 4s 427us/step - loss: 0.6732 - acc: 0.5751 - precision: 0.6019 - precision_1: 0.4146 - val_loss: 0.6921 - val_acc: 0.5479 - val_precision: 0.5876 - val_precision_1: 0.3315\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.68620\n",
      "Epoch 150/200\n",
      "9571/9571 [==============================] - 4s 433us/step - loss: 0.6726 - acc: 0.5716 - precision: 0.5940 - precision_1: 0.5166 - val_loss: 0.6916 - val_acc: 0.5442 - val_precision: 0.5303 - val_precision_1: 0.3340\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.68620\n",
      "Epoch 151/200\n",
      "9571/9571 [==============================] - 4s 392us/step - loss: 0.6739 - acc: 0.5761 - precision: 0.5849 - precision_1: 0.4989 - val_loss: 0.6912 - val_acc: 0.5451 - val_precision: 0.5668 - val_precision_1: 0.2965\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.68620\n",
      "Epoch 152/200\n",
      "9571/9571 [==============================] - 4s 384us/step - loss: 0.6726 - acc: 0.5714 - precision: 0.5704 - precision_1: 0.5086 - val_loss: 0.6912 - val_acc: 0.5564 - val_precision: 0.5465 - val_precision_1: 0.3003\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.68620\n",
      "Epoch 153/200\n",
      "9571/9571 [==============================] - 4s 389us/step - loss: 0.6742 - acc: 0.5787 - precision: 0.5983 - precision_1: 0.4698 - val_loss: 0.6912 - val_acc: 0.5498 - val_precision: 0.5540 - val_precision_1: 0.3083\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.68620\n",
      "Epoch 154/200\n",
      "9571/9571 [==============================] - 4s 383us/step - loss: 0.6723 - acc: 0.5757 - precision: 0.5948 - precision_1: 0.5006 - val_loss: 0.6924 - val_acc: 0.5414 - val_precision: 0.6065 - val_precision_1: 0.2481\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.68620\n",
      "Epoch 155/200\n",
      "9571/9571 [==============================] - 4s 389us/step - loss: 0.6720 - acc: 0.5765 - precision: 0.5826 - precision_1: 0.4809 - val_loss: 0.6919 - val_acc: 0.5564 - val_precision: 0.5624 - val_precision_1: 0.3183\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.68620\n",
      "Epoch 156/200\n",
      "9571/9571 [==============================] - 4s 391us/step - loss: 0.6726 - acc: 0.5765 - precision: 0.6446 - precision_1: 0.5342 - val_loss: 0.6915 - val_acc: 0.5508 - val_precision: 0.5799 - val_precision_1: 0.2506\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.68620\n",
      "Epoch 157/200\n",
      "9571/9571 [==============================] - 4s 389us/step - loss: 0.6714 - acc: 0.5806 - precision: 0.6140 - precision_1: 0.5786 - val_loss: 0.6920 - val_acc: 0.5451 - val_precision: 0.5839 - val_precision_1: 0.3409\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.68620\n",
      "Epoch 158/200\n",
      "9571/9571 [==============================] - 4s 384us/step - loss: 0.6714 - acc: 0.5868 - precision: 0.6278 - precision_1: 0.4768 - val_loss: 0.6926 - val_acc: 0.5479 - val_precision: 0.6219 - val_precision_1: 0.2607\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.68620\n",
      "Epoch 159/200\n",
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6733 - acc: 0.5767 - precision: 0.5691 - precision_1: 0.5084 - val_loss: 0.6935 - val_acc: 0.5508 - val_precision: 0.6483 - val_precision_1: 0.2927\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.68620\n",
      "Epoch 160/200\n",
      "9571/9571 [==============================] - 4s 384us/step - loss: 0.6707 - acc: 0.5767 - precision: 0.5954 - precision_1: 0.5348 - val_loss: 0.6943 - val_acc: 0.5432 - val_precision: 0.6176 - val_precision_1: 0.2764\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.68620\n",
      "Epoch 161/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6716 - acc: 0.5810 - precision: 0.5838 - precision_1: 0.5267 - val_loss: 0.6937 - val_acc: 0.5479 - val_precision: 0.6142 - val_precision_1: 0.2607\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.68620\n",
      "Epoch 162/200\n",
      "9571/9571 [==============================] - 4s 388us/step - loss: 0.6709 - acc: 0.5783 - precision: 0.6077 - precision_1: 0.4862 - val_loss: 0.6937 - val_acc: 0.5423 - val_precision: 0.4987 - val_precision_1: 0.3308\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.68620\n",
      "Epoch 163/200\n",
      "9571/9571 [==============================] - 4s 387us/step - loss: 0.6706 - acc: 0.5757 - precision: 0.6317 - precision_1: 0.5450 - val_loss: 0.6941 - val_acc: 0.5357 - val_precision: 0.5893 - val_precision_1: 0.2105\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.00021870001510251313.\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.68620\n",
      "Epoch 164/200\n",
      "9571/9571 [==============================] - 4s 384us/step - loss: 0.6721 - acc: 0.5807 - precision: 0.5905 - precision_1: 0.5193 - val_loss: 0.6944 - val_acc: 0.5404 - val_precision: 0.5335 - val_precision_1: 0.2145\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.68620\n",
      "Epoch 165/200\n",
      "9571/9571 [==============================] - 4s 390us/step - loss: 0.6736 - acc: 0.5705 - precision: 0.6049 - precision_1: 0.4996 - val_loss: 0.6943 - val_acc: 0.5498 - val_precision: 0.5904 - val_precision_1: 0.2105\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.68620\n",
      "Epoch 166/200\n",
      "9571/9571 [==============================] - 4s 390us/step - loss: 0.6707 - acc: 0.5799 - precision: 0.6245 - precision_1: 0.5633 - val_loss: 0.6943 - val_acc: 0.5282 - val_precision: 0.5545 - val_precision_1: 0.3774\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.68620\n",
      "Epoch 167/200\n",
      "9571/9571 [==============================] - 4s 414us/step - loss: 0.6688 - acc: 0.5883 - precision: 0.6013 - precision_1: 0.5330 - val_loss: 0.6941 - val_acc: 0.5395 - val_precision: 0.5259 - val_precision_1: 0.3767\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.68620\n",
      "Epoch 168/200\n",
      "9571/9571 [==============================] - 4s 409us/step - loss: 0.6702 - acc: 0.5790 - precision: 0.5793 - precision_1: 0.5522 - val_loss: 0.6935 - val_acc: 0.5348 - val_precision: 0.5731 - val_precision_1: 0.4074\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.68620\n",
      "Epoch 169/200\n",
      "9571/9571 [==============================] - 4s 390us/step - loss: 0.6691 - acc: 0.5795 - precision: 0.6138 - precision_1: 0.5470 - val_loss: 0.6941 - val_acc: 0.5338 - val_precision: 0.5691 - val_precision_1: 0.3473\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.68620\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6712 - acc: 0.5817 - precision: 0.6122 - precision_1: 0.5510 - val_loss: 0.6932 - val_acc: 0.5461 - val_precision: 0.5712 - val_precision_1: 0.4326\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.68620\n",
      "Epoch 171/200\n",
      "9571/9571 [==============================] - 4s 391us/step - loss: 0.6695 - acc: 0.5813 - precision: 0.5901 - precision_1: 0.5924 - val_loss: 0.6933 - val_acc: 0.5432 - val_precision: 0.5441 - val_precision_1: 0.3366\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.68620\n",
      "Epoch 172/200\n",
      "9571/9571 [==============================] - 4s 421us/step - loss: 0.6714 - acc: 0.5796 - precision: 0.5976 - precision_1: 0.5578 - val_loss: 0.6929 - val_acc: 0.5357 - val_precision: 0.5865 - val_precision_1: 0.2764\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.68620\n",
      "Epoch 173/200\n",
      "9571/9571 [==============================] - 4s 391us/step - loss: 0.6693 - acc: 0.5830 - precision: 0.6063 - precision_1: 0.5832 - val_loss: 0.6928 - val_acc: 0.5423 - val_precision: 0.5230 - val_precision_1: 0.3358\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.68620\n",
      "Epoch 174/200\n",
      "9571/9571 [==============================] - 4s 403us/step - loss: 0.6720 - acc: 0.5813 - precision: 0.5988 - precision_1: 0.5656 - val_loss: 0.6932 - val_acc: 0.5404 - val_precision: 0.5744 - val_precision_1: 0.2864\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.68620\n",
      "Epoch 175/200\n",
      "9571/9571 [==============================] - 4s 412us/step - loss: 0.6667 - acc: 0.5890 - precision: 0.6138 - precision_1: 0.5556 - val_loss: 0.6937 - val_acc: 0.5442 - val_precision: 0.5635 - val_precision_1: 0.3108\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.68620\n",
      "Epoch 176/200\n",
      "9571/9571 [==============================] - 4s 403us/step - loss: 0.6698 - acc: 0.5802 - precision: 0.5841 - precision_1: 0.5343 - val_loss: 0.6936 - val_acc: 0.5489 - val_precision: 0.5599 - val_precision_1: 0.4425\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.68620\n",
      "Epoch 177/200\n",
      "9571/9571 [==============================] - 4s 426us/step - loss: 0.6690 - acc: 0.5814 - precision: 0.6154 - precision_1: 0.5866 - val_loss: 0.6955 - val_acc: 0.5451 - val_precision: 0.5783 - val_precision_1: 0.2907\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.68620\n",
      "Epoch 178/200\n",
      "9571/9571 [==============================] - 4s 428us/step - loss: 0.6673 - acc: 0.5863 - precision: 0.6010 - precision_1: 0.6129 - val_loss: 0.6954 - val_acc: 0.5432 - val_precision: 0.5630 - val_precision_1: 0.3148\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.68620\n",
      "Epoch 179/200\n",
      "9571/9571 [==============================] - 4s 443us/step - loss: 0.6675 - acc: 0.5907 - precision: 0.6264 - precision_1: 0.6105 - val_loss: 0.6943 - val_acc: 0.5385 - val_precision: 0.5394 - val_precision_1: 0.4421\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.68620\n",
      "Epoch 180/200\n",
      "9571/9571 [==============================] - 4s 386us/step - loss: 0.6684 - acc: 0.5885 - precision: 0.6082 - precision_1: 0.5529 - val_loss: 0.6947 - val_acc: 0.5357 - val_precision: 0.5541 - val_precision_1: 0.3824\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.68620\n",
      "Epoch 181/200\n",
      "9571/9571 [==============================] - 4s 411us/step - loss: 0.6686 - acc: 0.5848 - precision: 0.6093 - precision_1: 0.5957 - val_loss: 0.6952 - val_acc: 0.5479 - val_precision: 0.5713 - val_precision_1: 0.3559\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.68620\n",
      "Epoch 182/200\n",
      "9571/9571 [==============================] - 4s 405us/step - loss: 0.6687 - acc: 0.5832 - precision: 0.6256 - precision_1: 0.6108 - val_loss: 0.6948 - val_acc: 0.5414 - val_precision: 0.5408 - val_precision_1: 0.4074\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.68620\n",
      "Epoch 183/200\n",
      "9571/9571 [==============================] - 4s 392us/step - loss: 0.6695 - acc: 0.5809 - precision: 0.5949 - precision_1: 0.5869 - val_loss: 0.6948 - val_acc: 0.5348 - val_precision: 0.5382 - val_precision_1: 0.4268\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.68620\n",
      "Epoch 184/200\n",
      "9571/9571 [==============================] - 4s 410us/step - loss: 0.6691 - acc: 0.5900 - precision: 0.5992 - precision_1: 0.5484 - val_loss: 0.6950 - val_acc: 0.5404 - val_precision: 0.5828 - val_precision_1: 0.4318\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.68620\n",
      "Epoch 185/200\n",
      "9571/9571 [==============================] - 4s 423us/step - loss: 0.6683 - acc: 0.5836 - precision: 0.5975 - precision_1: 0.5978 - val_loss: 0.6951 - val_acc: 0.5451 - val_precision: 0.5459 - val_precision_1: 0.3817\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.68620\n",
      "Epoch 186/200\n",
      "9571/9571 [==============================] - 4s 416us/step - loss: 0.6688 - acc: 0.5831 - precision: 0.5878 - precision_1: 0.5857 - val_loss: 0.6946 - val_acc: 0.5461 - val_precision: 0.5779 - val_precision_1: 0.3516\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.68620\n",
      "Epoch 187/200\n",
      "9571/9571 [==============================] - 4s 412us/step - loss: 0.6674 - acc: 0.5927 - precision: 0.6059 - precision_1: 0.5854 - val_loss: 0.6946 - val_acc: 0.5395 - val_precision: 0.5520 - val_precision_1: 0.3509\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.68620\n",
      "Epoch 188/200\n",
      "9571/9571 [==============================] - 4s 391us/step - loss: 0.6669 - acc: 0.5885 - precision: 0.6245 - precision_1: 0.6067 - val_loss: 0.6949 - val_acc: 0.5348 - val_precision: 0.5740 - val_precision_1: 0.4161\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.68620\n",
      "Epoch 189/200\n",
      "9571/9571 [==============================] - 4s 451us/step - loss: 0.6663 - acc: 0.5899 - precision: 0.5991 - precision_1: 0.6091 - val_loss: 0.6948 - val_acc: 0.5385 - val_precision: 0.5304 - val_precision_1: 0.3673\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.68620\n",
      "Epoch 190/200\n",
      "9571/9571 [==============================] - 4s 420us/step - loss: 0.6661 - acc: 0.5832 - precision: 0.6286 - precision_1: 0.6605 - val_loss: 0.6960 - val_acc: 0.5367 - val_precision: 0.5486 - val_precision_1: 0.4619\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.68620\n",
      "Epoch 191/200\n",
      "9571/9571 [==============================] - 4s 399us/step - loss: 0.6677 - acc: 0.5840 - precision: 0.6020 - precision_1: 0.5513 - val_loss: 0.6950 - val_acc: 0.5357 - val_precision: 0.5192 - val_precision_1: 0.5060\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.68620\n",
      "Epoch 192/200\n",
      "9571/9571 [==============================] - 4s 409us/step - loss: 0.6665 - acc: 0.5885 - precision: 0.5832 - precision_1: 0.6485 - val_loss: 0.6965 - val_acc: 0.5357 - val_precision: 0.5387 - val_precision_1: 0.4067\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.68620\n",
      "Epoch 193/200\n",
      "9571/9571 [==============================] - 4s 426us/step - loss: 0.6661 - acc: 0.5895 - precision: 0.6094 - precision_1: 0.6182 - val_loss: 0.6966 - val_acc: 0.5357 - val_precision: 0.5082 - val_precision_1: 0.4468\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.0001968300188309513.\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.68620\n",
      "Epoch 194/200\n",
      "9571/9571 [==============================] - 4s 438us/step - loss: 0.6653 - acc: 0.5861 - precision: 0.6236 - precision_1: 0.6260 - val_loss: 0.6969 - val_acc: 0.5291 - val_precision: 0.5384 - val_precision_1: 0.4826\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.68620\n",
      "Epoch 195/200\n",
      "9571/9571 [==============================] - 4s 400us/step - loss: 0.6635 - acc: 0.5892 - precision: 0.5938 - precision_1: 0.6431 - val_loss: 0.6976 - val_acc: 0.5320 - val_precision: 0.5511 - val_precision_1: 0.4726\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.68620\n",
      "Epoch 196/200\n",
      "9571/9571 [==============================] - 4s 395us/step - loss: 0.6658 - acc: 0.5898 - precision: 0.5890 - precision_1: 0.5881 - val_loss: 0.6965 - val_acc: 0.5357 - val_precision: 0.5288 - val_precision_1: 0.4619\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.68620\n",
      "Epoch 197/200\n",
      "9571/9571 [==============================] - 4s 426us/step - loss: 0.6662 - acc: 0.5878 - precision: 0.6065 - precision_1: 0.6532 - val_loss: 0.6956 - val_acc: 0.5367 - val_precision: 0.5529 - val_precision_1: 0.4024\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.68620\n",
      "Epoch 198/200\n",
      "9571/9571 [==============================] - 4s 413us/step - loss: 0.6665 - acc: 0.5870 - precision: 0.5910 - precision_1: 0.6059 - val_loss: 0.6951 - val_acc: 0.5338 - val_precision: 0.5437 - val_precision_1: 0.5581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00198: val_loss did not improve from 0.68620\n",
      "Epoch 199/200\n",
      "9571/9571 [==============================] - 4s 408us/step - loss: 0.6657 - acc: 0.5902 - precision: 0.6058 - precision_1: 0.6174 - val_loss: 0.6956 - val_acc: 0.5385 - val_precision: 0.5450 - val_precision_1: 0.4876\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.68620\n",
      "Epoch 200/200\n",
      "9571/9571 [==============================] - 4s 406us/step - loss: 0.6639 - acc: 0.5944 - precision: 0.6246 - precision_1: 0.6288 - val_loss: 0.6957 - val_acc: 0.5442 - val_precision: 0.5609 - val_precision_1: 0.4729\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.68620\n"
     ]
    }
   ],
   "source": [
    "def precision_threshold(threshold=0.5):\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Computes the precision over the whole batch using threshold_value.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_true = y_true[:, 0]\n",
    "        y_pred = y_pred[:, 0]\n",
    "        \n",
    "        threshold_value = threshold\n",
    "        # Adaptation of the \"round()\" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.\n",
    "        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "        # count the predicted positives\n",
    "        predicted_positives = K.sum(y_pred)\n",
    "        # Get the precision ratio\n",
    "        precision_ratio = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision_ratio\n",
    "    return precision\n",
    "\n",
    "opt = Nadam(lr=0.0003)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=30, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', precision_threshold(0.55), precision_threshold(0.6)])\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "          epochs = 200, \n",
    "          batch_size = 64, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[reduce_lr, checkpointer],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.title('precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33264463 0.66735537]\n",
      " [0.27931034 0.72068966]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#model.load_weights(\"model.hdf5\")\n",
    "pred = model.predict(np.array(X_test), batch_size=64)\n",
    "\n",
    "C = confusion_matrix([np.argmax(y) for y in Y_test], [np.argmax(y) for y in pred])\n",
    "\n",
    "print (C / C.astype(np.float).sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[161, 323],\n",
       "       [162, 418]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.dtype' object has no attribute 'base_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-51a040fa91ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(x, min_value, max_value)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0mmin_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m     \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.dtype' object has no attribute 'base_dtype'"
     ]
    }
   ],
   "source": [
    "K.clip(pred[0] * Y_test[0], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fba19a4beb8>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([np.argmax(y) for y in Y_test])\n",
    "pd.DataFrame(pred)[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227844, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[43650,:]\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.094520</td>\n",
       "      <td>0.905480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126728</td>\n",
       "      <td>0.873272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283482</td>\n",
       "      <td>0.716518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.372990</td>\n",
       "      <td>0.627011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.404254</td>\n",
       "      <td>0.595747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.418198</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.773934</td>\n",
       "      <td>0.226066</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938506</td>\n",
       "      <td>0.061494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995280</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1    2    3\n",
       "8  0.094520  0.905480  0.0  1.0\n",
       "2  0.126728  0.873272  0.0  1.0\n",
       "3  0.283482  0.716518  0.0  1.0\n",
       "1  0.372990  0.627011  0.0  1.0\n",
       "7  0.404254  0.595747  0.0  1.0\n",
       "5  0.418198  0.581802  1.0  0.0\n",
       "9  0.773934  0.226066  1.0  0.0\n",
       "0  0.938506  0.061494  1.0  0.0\n",
       "4  0.995280  0.004720  1.0  0.0\n",
       "6  0.997012  0.002988  1.0  0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(np.array(X_train), batch_size=64)\n",
    "df = pd.DataFrame(np.concatenate((pred, Y_train), axis=1))\n",
    "df[(df[0]>.6)&(df[2]==1)].shape\n",
    "df[(df[0]>.6)].shape\n",
    "\n",
    "#df.shape\n",
    "df.sort_values(0,axis=0,ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-6.85796610e-01, -6.87730965e-01, -6.86905089e-01,\n",
       "         -6.87830125e-01, -6.88255899e-01, -7.22363962e-01,\n",
       "         -6.91513056e-01],\n",
       "        [-6.88337308e-01, -6.89255374e-01, -6.87921389e-01,\n",
       "         -6.88846406e-01, -6.88309303e-01, -7.07106781e-01,\n",
       "         -7.07106781e-01],\n",
       "        [-6.88845448e-01, -6.88239102e-01, -6.87413239e-01,\n",
       "         -6.87321985e-01, -6.88212421e-01, -6.91513056e-01,\n",
       "         -7.22363962e-01],\n",
       "        [-6.86812889e-01, -6.83665874e-01, -6.85380640e-01,\n",
       "         -6.83765003e-01, -6.87785964e-01, -6.75590208e-01,\n",
       "         -7.37277337e-01],\n",
       "        [-6.83764052e-01, -6.85190283e-01, -6.84364341e-01,\n",
       "         -6.85289424e-01, -6.87545322e-01, -6.59345815e-01,\n",
       "         -7.51839807e-01],\n",
       "        [-6.85796610e-01, -6.86206556e-01, -6.84364341e-01,\n",
       "         -6.85289424e-01, -6.87327599e-01, -6.42787610e-01,\n",
       "         -7.66044443e-01],\n",
       "        [-6.85288470e-01, -6.85190283e-01, -6.84364341e-01,\n",
       "         -6.84781284e-01, -6.87082211e-01, -6.25923472e-01,\n",
       "         -7.79884483e-01],\n",
       "        [-6.84780331e-01, -6.86206556e-01, -6.85888790e-01,\n",
       "         -6.87321985e-01, -6.87102194e-01, -6.08761429e-01,\n",
       "         -7.93353340e-01],\n",
       "        [-6.87321029e-01, -6.87730965e-01, -6.87921389e-01,\n",
       "         -6.89354546e-01, -6.87313875e-01, -5.91309648e-01,\n",
       "         -8.06444604e-01],\n",
       "        [-6.89353587e-01, -6.90779784e-01, -6.91478438e-01,\n",
       "         -6.92911528e-01, -6.87844198e-01, -5.73576436e-01,\n",
       "         -8.19152044e-01],\n",
       "        [-6.93418704e-01, -6.94844875e-01, -6.94527336e-01,\n",
       "         -6.94435949e-01, -6.88469214e-01, -5.55570233e-01,\n",
       "         -8.31469612e-01],\n",
       "        [-6.94434984e-01, -6.94336738e-01, -6.93002887e-01,\n",
       "         -6.92403388e-01, -6.88841104e-01, -5.37299608e-01,\n",
       "         -8.43391446e-01]],\n",
       "\n",
       "       [[-6.92910565e-01, -6.94336738e-01, -6.91986587e-01,\n",
       "         -6.92911528e-01, -6.89574193e-01, -5.00000000e-01,\n",
       "         -8.66025404e-01],\n",
       "        [-6.92910565e-01, -6.92812329e-01, -6.90970288e-01,\n",
       "         -6.92403388e-01, -6.89840847e-01, -4.80988769e-01,\n",
       "         -8.76726756e-01],\n",
       "        [-6.92402425e-01, -6.91796056e-01, -6.90970288e-01,\n",
       "         -6.90370827e-01, -6.89888504e-01, -4.61748613e-01,\n",
       "         -8.87010833e-01],\n",
       "        [-6.90369867e-01, -6.89763511e-01, -6.89445838e-01,\n",
       "         -6.88338266e-01, -6.89738022e-01, -4.42288690e-01,\n",
       "         -8.96872742e-01],\n",
       "        [-6.88337308e-01, -6.88747238e-01, -6.87921389e-01,\n",
       "         -6.89354546e-01, -6.89698672e-01, -4.22618262e-01,\n",
       "         -9.06307787e-01],\n",
       "        [-6.89353587e-01, -6.90779784e-01, -6.88937689e-01,\n",
       "         -6.89862687e-01, -6.89711469e-01, -4.02746690e-01,\n",
       "         -9.15311479e-01],\n",
       "        [-6.89861727e-01, -6.91287920e-01, -6.89445838e-01,\n",
       "         -6.90370827e-01, -6.89771448e-01, -3.82683432e-01,\n",
       "         -9.23879533e-01],\n",
       "        [-6.90369867e-01, -6.90779784e-01, -6.88937689e-01,\n",
       "         -6.89354546e-01, -6.89728915e-01, -3.62438038e-01,\n",
       "         -9.32007869e-01],\n",
       "        [-6.89353587e-01, -6.88239102e-01, -6.87921389e-01,\n",
       "         -6.88846406e-01, -6.89642032e-01, -3.42020143e-01,\n",
       "         -9.39692621e-01],\n",
       "        [-6.88845448e-01, -6.89763511e-01, -6.87921389e-01,\n",
       "         -6.89354546e-01, -6.89611823e-01, -3.21439465e-01,\n",
       "         -9.46930129e-01],\n",
       "        [-6.89353587e-01, -6.90271647e-01, -6.88429539e-01,\n",
       "         -6.89354546e-01, -6.89584492e-01, -3.00705800e-01,\n",
       "         -9.53716951e-01],\n",
       "        [-6.89353587e-01, -6.88747238e-01, -6.87921389e-01,\n",
       "         -6.86813845e-01, -6.89317763e-01, -2.79829014e-01,\n",
       "         -9.60049854e-01]],\n",
       "\n",
       "       [[-6.86812889e-01, -6.87730965e-01, -6.85380640e-01,\n",
       "         -6.87321985e-01, -6.88411350e-01, -1.95090322e-01,\n",
       "         -9.80785280e-01],\n",
       "        [-6.86812889e-01, -6.87222829e-01, -6.85888790e-01,\n",
       "         -6.85289424e-01, -6.88111148e-01, -1.73648178e-01,\n",
       "         -9.84807753e-01],\n",
       "        [-6.85288470e-01, -6.86206556e-01, -6.84364341e-01,\n",
       "         -6.85289424e-01, -6.87839537e-01, -1.52123386e-01,\n",
       "         -9.88361510e-01],\n",
       "        [-6.85288470e-01, -6.86714692e-01, -6.85380640e-01,\n",
       "         -6.86305705e-01, -6.87690593e-01, -1.30526192e-01,\n",
       "         -9.91444861e-01],\n",
       "        [-6.86812889e-01, -6.87222829e-01, -6.85380640e-01,\n",
       "         -6.87321985e-01, -6.87652636e-01, -1.08866875e-01,\n",
       "         -9.94056338e-01],\n",
       "        [-6.86812889e-01, -6.88747238e-01, -6.86396940e-01,\n",
       "         -6.88338266e-01, -6.87715093e-01, -8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [-6.88337308e-01, -6.89763511e-01, -6.88937689e-01,\n",
       "         -6.90370827e-01, -6.87965203e-01, -6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [-6.90369867e-01, -6.92304193e-01, -6.90970288e-01,\n",
       "         -6.90878967e-01, -6.88239894e-01, -4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [-6.90878006e-01, -6.91796056e-01, -6.90462138e-01,\n",
       "         -6.90878967e-01, -6.88488423e-01, -2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.90878006e-01, -6.92304193e-01, -6.90462138e-01,\n",
       "         -6.91895248e-01, -6.88810083e-01, -1.83697020e-16,\n",
       "         -1.00000000e+00],\n",
       "        [-6.92402425e-01, -6.92304193e-01, -6.90462138e-01,\n",
       "         -6.90878967e-01, -6.89004309e-01,  2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.91386146e-01, -6.89763511e-01, -6.90970288e-01,\n",
       "         -6.87830125e-01, -6.88889635e-01,  4.36193874e-02,\n",
       "         -9.99048222e-01]],\n",
       "\n",
       "       [[-6.86812889e-01, -6.88747238e-01, -6.86396940e-01,\n",
       "         -6.88338266e-01, -6.87715093e-01, -8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [-6.88337308e-01, -6.89763511e-01, -6.88937689e-01,\n",
       "         -6.90370827e-01, -6.87965203e-01, -6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [-6.90369867e-01, -6.92304193e-01, -6.90970288e-01,\n",
       "         -6.90878967e-01, -6.88239894e-01, -4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [-6.90878006e-01, -6.91796056e-01, -6.90462138e-01,\n",
       "         -6.90878967e-01, -6.88488423e-01, -2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.90878006e-01, -6.92304193e-01, -6.90462138e-01,\n",
       "         -6.91895248e-01, -6.88810083e-01, -1.83697020e-16,\n",
       "         -1.00000000e+00],\n",
       "        [-6.92402425e-01, -6.92304193e-01, -6.90462138e-01,\n",
       "         -6.90878967e-01, -6.89004309e-01,  2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.91386146e-01, -6.89763511e-01, -6.90970288e-01,\n",
       "         -6.87830125e-01, -6.88889635e-01,  4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [-6.88337308e-01, -6.90271647e-01, -6.87921389e-01,\n",
       "         -6.89862687e-01, -6.88979484e-01,  6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [-6.89861727e-01, -6.90779784e-01, -6.89445838e-01,\n",
       "         -6.91387107e-01, -6.89205976e-01,  8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [-6.91386146e-01, -6.92304193e-01, -6.90462138e-01,\n",
       "         -6.90878967e-01, -6.89362498e-01,  1.08866875e-01,\n",
       "         -9.94056338e-01],\n",
       "        [-6.90369867e-01, -6.92304193e-01, -6.89953988e-01,\n",
       "         -6.90878967e-01, -6.89504112e-01,  1.30526192e-01,\n",
       "         -9.91444861e-01],\n",
       "        [-6.90878006e-01, -6.90779784e-01, -6.88937689e-01,\n",
       "         -6.88846406e-01, -6.89438639e-01,  1.52123386e-01,\n",
       "         -9.88361510e-01]],\n",
       "\n",
       "       [[-6.92402425e-01, -6.93320465e-01, -6.91986587e-01,\n",
       "         -6.91895248e-01, -6.92087137e-01,  5.00000000e-01,\n",
       "         -8.66025404e-01],\n",
       "        [-6.92402425e-01, -6.92304193e-01, -6.91986587e-01,\n",
       "         -6.91387107e-01, -6.92017663e-01,  5.18773258e-01,\n",
       "         -8.54911871e-01],\n",
       "        [-6.90878006e-01, -6.91796056e-01, -6.89953988e-01,\n",
       "         -6.91387107e-01, -6.91954805e-01,  5.37299608e-01,\n",
       "         -8.43391446e-01],\n",
       "        [-6.90878006e-01, -6.92812329e-01, -6.90970288e-01,\n",
       "         -6.92911528e-01, -6.92043134e-01,  5.55570233e-01,\n",
       "         -8.31469612e-01],\n",
       "        [-6.92402425e-01, -6.92304193e-01, -6.90970288e-01,\n",
       "         -6.91387107e-01, -6.91977850e-01,  5.73576436e-01,\n",
       "         -8.19152044e-01],\n",
       "        [-6.90878006e-01, -6.91796056e-01, -6.90462138e-01,\n",
       "         -6.91387107e-01, -6.91918784e-01,  5.91309648e-01,\n",
       "         -8.06444604e-01],\n",
       "        [-6.91386146e-01, -6.92304193e-01, -6.89953988e-01,\n",
       "         -6.90878967e-01, -6.91816942e-01,  6.08761429e-01,\n",
       "         -7.93353340e-01],\n",
       "        [-6.90369867e-01, -6.91287920e-01, -6.89445838e-01,\n",
       "         -6.89862687e-01, -6.91628000e-01,  6.25923472e-01,\n",
       "         -7.79884483e-01],\n",
       "        [-6.90369867e-01, -6.91796056e-01, -6.88937689e-01,\n",
       "         -6.90370827e-01, -6.91505452e-01,  6.42787610e-01,\n",
       "         -7.66044443e-01],\n",
       "        [-6.90369867e-01, -6.90779784e-01, -6.89445838e-01,\n",
       "         -6.90370827e-01, -6.91394576e-01,  6.59345815e-01,\n",
       "         -7.51839807e-01],\n",
       "        [-6.90369867e-01, -6.91796056e-01, -6.89953988e-01,\n",
       "         -6.91895248e-01, -6.91439459e-01,  6.75590208e-01,\n",
       "         -7.37277337e-01],\n",
       "        [-6.91894286e-01, -6.91796056e-01, -6.90462138e-01,\n",
       "         -6.89862687e-01, -6.91286468e-01,  6.91513056e-01,\n",
       "         -7.22363962e-01]],\n",
       "\n",
       "       [[-6.72584980e-01, -6.74519419e-01, -6.72676896e-01,\n",
       "         -6.74110337e-01, -6.75097386e-01, -2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.74109399e-01, -6.75535692e-01, -6.72676896e-01,\n",
       "         -6.74110337e-01, -6.75000376e-01, -1.83697020e-16,\n",
       "         -1.00000000e+00],\n",
       "        [-6.74617538e-01, -6.76043828e-01, -6.73185045e-01,\n",
       "         -6.74618478e-01, -6.74961005e-01,  2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.75125678e-01, -6.74519419e-01, -6.73185045e-01,\n",
       "         -6.74110337e-01, -6.74876984e-01,  4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [-6.74109399e-01, -6.75535692e-01, -6.73185045e-01,\n",
       "         -6.74618478e-01, -6.74849365e-01,  6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [-6.74617538e-01, -6.74519419e-01, -6.72676896e-01,\n",
       "         -6.73602197e-01, -6.74727576e-01,  8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [-6.73601259e-01, -6.72995010e-01, -6.72676896e-01,\n",
       "         -6.72077776e-01, -6.74472185e-01,  1.08866875e-01,\n",
       "         -9.94056338e-01],\n",
       "        [-6.71568701e-01, -6.73503147e-01, -6.71152446e-01,\n",
       "         -6.72585917e-01, -6.74289517e-01,  1.30526192e-01,\n",
       "         -9.91444861e-01],\n",
       "        [-6.72584980e-01, -6.74011283e-01, -6.71660596e-01,\n",
       "         -6.73094057e-01, -6.74172647e-01,  1.52123386e-01,\n",
       "         -9.88361510e-01],\n",
       "        [-6.73093119e-01, -6.73503147e-01, -6.71152446e-01,\n",
       "         -6.72585917e-01, -6.74018507e-01,  1.73648178e-01,\n",
       "         -9.84807753e-01],\n",
       "        [-6.72584980e-01, -6.74011283e-01, -6.71152446e-01,\n",
       "         -6.72585917e-01, -6.73879047e-01,  1.95090322e-01,\n",
       "         -9.80785280e-01],\n",
       "        [-6.72076840e-01, -6.69946192e-01, -6.70644296e-01,\n",
       "         -6.68012654e-01, -6.73317267e-01,  2.16439614e-01,\n",
       "         -9.76296007e-01]],\n",
       "\n",
       "       [[-6.64962885e-01, -6.65881101e-01, -6.63530199e-01,\n",
       "         -6.64455672e-01, -6.66546484e-01,  5.73576436e-01,\n",
       "         -8.19152044e-01],\n",
       "        [-6.64962885e-01, -6.64864828e-01, -6.63022050e-01,\n",
       "         -6.64455672e-01, -6.66344242e-01,  5.91309648e-01,\n",
       "         -8.06444604e-01],\n",
       "        [-6.64454746e-01, -6.64864828e-01, -6.63022050e-01,\n",
       "         -6.64455672e-01, -6.66161261e-01,  6.08761429e-01,\n",
       "         -7.93353340e-01],\n",
       "        [-6.64454746e-01, -6.64864828e-01, -6.62513900e-01,\n",
       "         -6.63947531e-01, -6.65947307e-01,  6.25923472e-01,\n",
       "         -7.79884483e-01],\n",
       "        [-6.63946606e-01, -6.65372964e-01, -6.64038349e-01,\n",
       "         -6.65471952e-01, -6.65898929e-01,  6.42787610e-01,\n",
       "         -7.66044443e-01],\n",
       "        [-6.65471025e-01, -6.65881101e-01, -6.64038349e-01,\n",
       "         -6.64963812e-01, -6.65806759e-01,  6.59345815e-01,\n",
       "         -7.51839807e-01],\n",
       "        [-6.64962885e-01, -6.66389237e-01, -6.64546499e-01,\n",
       "         -6.65980093e-01, -6.65820168e-01,  6.75590208e-01,\n",
       "         -7.37277337e-01],\n",
       "        [-6.65979165e-01, -6.67405510e-01, -6.65054649e-01,\n",
       "         -6.66488233e-01, -6.65880699e-01,  6.91513056e-01,\n",
       "         -7.22363962e-01],\n",
       "        [-6.65979165e-01, -6.66389237e-01, -6.64546499e-01,\n",
       "         -6.64963812e-01, -6.65790265e-01,  7.07106781e-01,\n",
       "         -7.07106781e-01],\n",
       "        [-6.64962885e-01, -6.66389237e-01, -6.63530199e-01,\n",
       "         -6.64963812e-01, -6.65708444e-01,  7.22363962e-01,\n",
       "         -6.91513056e-01],\n",
       "        [-6.64962885e-01, -6.66897374e-01, -6.64038349e-01,\n",
       "         -6.65980093e-01, -6.65731216e-01,  7.37277337e-01,\n",
       "         -6.75590208e-01],\n",
       "        [-6.65979165e-01, -6.65881101e-01, -6.64038349e-01,\n",
       "         -6.63947531e-01, -6.65558218e-01,  7.51839807e-01,\n",
       "         -6.59345815e-01]],\n",
       "\n",
       "       [[-6.59373350e-01, -6.61307873e-01, -6.58956851e-01,\n",
       "         -6.59374269e-01, -6.61224207e-01, -2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.59881489e-01, -6.60291601e-01, -6.58448702e-01,\n",
       "         -6.58357988e-01, -6.60948046e-01, -1.83697020e-16,\n",
       "         -1.00000000e+00],\n",
       "        [-6.58357070e-01, -6.60291601e-01, -6.57432402e-01,\n",
       "         -6.58866129e-01, -6.60746586e-01,  2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [-6.58357070e-01, -6.60291601e-01, -6.57940552e-01,\n",
       "         -6.58866129e-01, -6.60564313e-01,  4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [-6.58865210e-01, -6.59275328e-01, -6.58448702e-01,\n",
       "         -6.59882409e-01, -6.60496200e-01,  6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [-6.59881489e-01, -6.61307873e-01, -6.58448702e-01,\n",
       "         -6.59882409e-01, -6.60434573e-01,  8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [-6.59881489e-01, -6.59783464e-01, -6.57940552e-01,\n",
       "         -6.58357988e-01, -6.60233616e-01,  1.08866875e-01,\n",
       "         -9.94056338e-01],\n",
       "        [-6.57848931e-01, -6.59783464e-01, -6.58448702e-01,\n",
       "         -6.59882409e-01, -6.60196997e-01,  1.30526192e-01,\n",
       "         -9.91444861e-01],\n",
       "        [-6.59373350e-01, -6.60291601e-01, -6.57940552e-01,\n",
       "         -6.58866129e-01, -6.60067066e-01,  1.52123386e-01,\n",
       "         -9.88361510e-01],\n",
       "        [-6.58865210e-01, -6.57242782e-01, -6.57940552e-01,\n",
       "         -6.55817287e-01, -6.59659108e-01,  1.73648178e-01,\n",
       "         -9.84807753e-01],\n",
       "        [-6.55308233e-01, -6.57242782e-01, -6.54891653e-01,\n",
       "         -6.56325427e-01, -6.59338403e-01,  1.95090322e-01,\n",
       "         -9.80785280e-01],\n",
       "        [-6.56832651e-01, -6.56226509e-01, -6.54891653e-01,\n",
       "         -6.54292866e-01, -6.58854641e-01,  2.16439614e-01,\n",
       "         -9.76296007e-01]],\n",
       "\n",
       "       [[-6.57340791e-01, -6.58767191e-01, -6.56416102e-01,\n",
       "         -6.57849848e-01, -6.57675187e-01,  7.51839807e-01,\n",
       "         -6.59345815e-01],\n",
       "        [-6.65979165e-01, -6.66897374e-01, -6.65562799e-01,\n",
       "         -6.66488233e-01, -6.58511431e-01, -7.79884483e-01,\n",
       "         -6.25923472e-01],\n",
       "        [-6.65979165e-01, -6.66389237e-01, -6.64546499e-01,\n",
       "         -6.64963812e-01, -6.59122832e-01, -7.66044443e-01,\n",
       "         -6.42787610e-01],\n",
       "        [-6.64962885e-01, -6.65372964e-01, -6.64038349e-01,\n",
       "         -6.64455672e-01, -6.59627604e-01, -7.51839807e-01,\n",
       "         -6.59345815e-01],\n",
       "        [-6.63946606e-01, -6.65881101e-01, -6.64038349e-01,\n",
       "         -6.64963812e-01, -6.60132703e-01, -7.37277337e-01,\n",
       "         -6.75590208e-01],\n",
       "        [-6.65471025e-01, -6.66897374e-01, -6.67087248e-01,\n",
       "         -6.68012654e-01, -6.60880099e-01, -7.22363962e-01,\n",
       "         -6.91513056e-01],\n",
       "        [-6.67503584e-01, -6.67913646e-01, -6.66579098e-01,\n",
       "         -6.65980093e-01, -6.61362713e-01, -7.07106781e-01,\n",
       "         -7.07106781e-01],\n",
       "        [-6.65979165e-01, -6.64864828e-01, -6.65054649e-01,\n",
       "         -6.63947531e-01, -6.61605763e-01, -6.91513056e-01,\n",
       "         -7.22363962e-01],\n",
       "        [-6.63946606e-01, -6.65881101e-01, -6.64546499e-01,\n",
       "         -6.65980093e-01, -6.62019266e-01, -6.75590208e-01,\n",
       "         -7.37277337e-01],\n",
       "        [-6.65471025e-01, -6.65372964e-01, -6.63530199e-01,\n",
       "         -6.64455672e-01, -6.62248188e-01, -6.59345815e-01,\n",
       "         -7.51839807e-01],\n",
       "        [-6.64454746e-01, -6.64356692e-01, -6.63022050e-01,\n",
       "         -6.64455672e-01, -6.62455307e-01, -6.42787610e-01,\n",
       "         -7.66044443e-01],\n",
       "        [-6.64454746e-01, -6.63848555e-01, -6.63530199e-01,\n",
       "         -6.61914970e-01, -6.62400700e-01, -6.25923472e-01,\n",
       "         -7.79884483e-01]],\n",
       "\n",
       "       [[-6.64454746e-01, -6.63848555e-01, -6.63530199e-01,\n",
       "         -6.61914970e-01, -6.62400700e-01, -6.25923472e-01,\n",
       "         -7.79884483e-01],\n",
       "        [-6.62422187e-01, -6.64356692e-01, -6.63022050e-01,\n",
       "         -6.64455672e-01, -6.62593294e-01, -6.08761429e-01,\n",
       "         -7.93353340e-01],\n",
       "        [-6.63946606e-01, -6.64356692e-01, -6.63022050e-01,\n",
       "         -6.64455672e-01, -6.62767546e-01, -5.91309648e-01,\n",
       "         -8.06444604e-01],\n",
       "        [-6.64454746e-01, -6.62832282e-01, -6.63022050e-01,\n",
       "         -6.61914970e-01, -6.62683202e-01, -5.73576436e-01,\n",
       "         -8.19152044e-01],\n",
       "        [-6.62422187e-01, -6.63848555e-01, -6.62005750e-01,\n",
       "         -6.63947531e-01, -6.62800491e-01, -5.55570233e-01,\n",
       "         -8.31469612e-01],\n",
       "        [-6.63946606e-01, -6.65372964e-01, -6.63022050e-01,\n",
       "         -6.63947531e-01, -6.62906610e-01, -5.37299608e-01,\n",
       "         -8.43391446e-01],\n",
       "        [-6.63946606e-01, -6.65372964e-01, -6.63530199e-01,\n",
       "         -6.65471952e-01, -6.63147823e-01, -5.18773258e-01,\n",
       "         -8.54911871e-01],\n",
       "        [-6.65471025e-01, -6.66389237e-01, -6.63530199e-01,\n",
       "         -6.64455672e-01, -6.63269263e-01, -5.00000000e-01,\n",
       "         -8.66025404e-01],\n",
       "        [-6.64454746e-01, -6.66389237e-01, -6.65054649e-01,\n",
       "         -6.65980093e-01, -6.63524338e-01, -4.80988769e-01,\n",
       "         -8.76726756e-01],\n",
       "        [-6.65471025e-01, -6.67405510e-01, -6.66579098e-01,\n",
       "         -6.66996373e-01, -6.63851920e-01, -4.61748613e-01,\n",
       "         -8.87010833e-01],\n",
       "        [-6.67503584e-01, -6.68929919e-01, -6.66070948e-01,\n",
       "         -6.66996373e-01, -6.64148304e-01, -4.42288690e-01,\n",
       "         -8.96872742e-01],\n",
       "        [-6.67503584e-01, -6.67405510e-01, -6.66070948e-01,\n",
       "         -6.65471952e-01, -6.64271260e-01, -4.22618262e-01,\n",
       "         -9.06307787e-01]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83673469, 0.15855573, 0.00470958],\n",
       "       [0.78915663, 0.20481928, 0.0060241 ],\n",
       "       [0.86335404, 0.13043478, 0.00621118]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C / C.astype(np.float).sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55539956, 0.44460044])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = Y_train.sum(axis=0) / Y_train.shape[0]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4436597 , 0.5563404 ],\n",
       "       [0.5040323 , 0.49596766],\n",
       "       [0.5359964 , 0.4640036 ],\n",
       "       ...,\n",
       "       [0.57311577, 0.4268842 ],\n",
       "       [0.5362818 , 0.46371824],\n",
       "       [0.5139088 , 0.48609126]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.random.binomial(1, probs[1], pred.shape[0])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58421851 0.41578149]\n",
      " [0.59026688 0.40973312]]\n"
     ]
    }
   ],
   "source": [
    "C1 = confusion_matrix([np.argmax(y) for y in Y_test], s)\n",
    "print (C1 / C1.astype(np.float).sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4984567901234568"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([np.argmax(y) for y in Y_test] == s).sum() / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f61cf415860>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
