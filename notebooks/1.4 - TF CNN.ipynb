{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, TimeDistributed, Conv2D, MaxPooling1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tick Data and Create 5min RTH Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last</th>\n",
       "      <th>bid</th>\n",
       "      <th>ask</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:52.612000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:52.615000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:54.157000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:54.157000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-25 19:54:55.332000-05:00</th>\n",
       "      <td>2844.0</td>\n",
       "      <td>2843.75</td>\n",
       "      <td>2844.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    last      bid     ask  volume\n",
       "date                                                             \n",
       "2018-01-25 19:54:52.612000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:52.615000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:54.157000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:54.157000-05:00  2844.0  2843.75  2844.0       1\n",
       "2018-01-25 19:54:55.332000-05:00  2844.0  2843.75  2844.0       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tick_data = pd.read_feather('../data/processed/ES_tick.feather')\n",
    "tick_data = tick_data[tick_data['date'] > '2017-07-29']\n",
    "#Create Index from date column\n",
    "tick_data.index = tick_data['date']\n",
    "tick_data.drop(labels=['date'],axis=1,inplace=True)\n",
    "tick_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2017-08-01 09:35:00-04:00</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>2476.00</td>\n",
       "      <td>2472.50</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2470.908594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2017-08-01 09:40:00-04:00</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2471.50</td>\n",
       "      <td>2472.50</td>\n",
       "      <td>2471.060194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2017-08-01 09:45:00-04:00</td>\n",
       "      <td>2472.25</td>\n",
       "      <td>2473.25</td>\n",
       "      <td>2471.75</td>\n",
       "      <td>2473.00</td>\n",
       "      <td>2471.244978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-08-01 09:50:00-04:00</td>\n",
       "      <td>2473.00</td>\n",
       "      <td>2473.25</td>\n",
       "      <td>2472.00</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2471.388343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-08-01 09:55:00-04:00</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2473.00</td>\n",
       "      <td>2471.25</td>\n",
       "      <td>2471.25</td>\n",
       "      <td>2471.375165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date     open     high      low    close          ema\n",
       "81 2017-08-01 09:35:00-04:00  2475.50  2476.00  2472.50  2473.50  2470.908594\n",
       "82 2017-08-01 09:40:00-04:00  2473.50  2474.00  2471.50  2472.50  2471.060194\n",
       "83 2017-08-01 09:45:00-04:00  2472.25  2473.25  2471.75  2473.00  2471.244978\n",
       "84 2017-08-01 09:50:00-04:00  2473.00  2473.25  2472.00  2472.75  2471.388343\n",
       "85 2017-08-01 09:55:00-04:00  2472.75  2473.00  2471.25  2471.25  2471.375165"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resample to get 5min bars\n",
    "five_min_data = pd.DataFrame(\n",
    "    tick_data['last'].resample('5Min', loffset=datetime.timedelta(minutes=5)).ohlc())\n",
    "\n",
    "import pandas_market_calendars as mcal\n",
    "#We hack the NYSE Calendar extending the close until 4:15\n",
    "class CMERTHCalendar(mcal.exchange_calendar_nyse.NYSEExchangeCalendar):\n",
    "    @property\n",
    "    def close_time(self):\n",
    "        return datetime.time(16, 15)\n",
    "    \n",
    "#Create RTH Calendar\n",
    "nyse = CMERTHCalendar()\n",
    "schedule = nyse.schedule(start_date=five_min_data.index.min(), \n",
    "                         end_date=five_min_data.index.max())\n",
    "\n",
    "#Filter out those bars that occur during RTH\n",
    "five_min_data['dates'] = pd.to_datetime(five_min_data.index.to_datetime().date)\n",
    "five_min_data['valid_date'] = five_min_data['dates'].isin(schedule.index)\n",
    "five_min_data['valid_time'] = False\n",
    "during_rth = five_min_data['valid_date'] & \\\n",
    "        (five_min_data.index > schedule.loc[five_min_data['dates'],'market_open']) & \\\n",
    "        (five_min_data.index <= schedule.loc[five_min_data['dates'],'market_close'])\n",
    "five_min_data.loc[during_rth, 'valid_time'] = True\n",
    "five_min_data = five_min_data[five_min_data['valid_time'] == True]\n",
    "five_min_data.drop(['dates','valid_date','valid_time'], axis=1, inplace=True)\n",
    "\n",
    "#Add ema\n",
    "five_min_data['ema'] = five_min_data['close'].ewm(span=20, min_periods=20).mean()\n",
    "\n",
    "#Reset index\n",
    "five_min_data.reset_index(inplace=True)\n",
    "\n",
    "five_min_data[81:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>ema</th>\n",
       "      <th>sin_time</th>\n",
       "      <th>cos_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-31 09:35:00-04:00</td>\n",
       "      <td>2474.75</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.402747</td>\n",
       "      <td>-0.915311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-31 09:40:00-04:00</td>\n",
       "      <td>2475.25</td>\n",
       "      <td>2476.00</td>\n",
       "      <td>2473.75</td>\n",
       "      <td>2475.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.422618</td>\n",
       "      <td>-0.906308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-31 09:45:00-04:00</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2475.75</td>\n",
       "      <td>2474.50</td>\n",
       "      <td>2474.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.442289</td>\n",
       "      <td>-0.896873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-31 09:50:00-04:00</td>\n",
       "      <td>2474.50</td>\n",
       "      <td>2475.00</td>\n",
       "      <td>2473.50</td>\n",
       "      <td>2473.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.461749</td>\n",
       "      <td>-0.887011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-31 09:55:00-04:00</td>\n",
       "      <td>2474.00</td>\n",
       "      <td>2474.25</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>2472.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.480989</td>\n",
       "      <td>-0.876727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date     open     high      low    close  ema  \\\n",
       "0 2017-07-31 09:35:00-04:00  2474.75  2475.75  2474.00  2475.50  NaN   \n",
       "1 2017-07-31 09:40:00-04:00  2475.25  2476.00  2473.75  2475.50  NaN   \n",
       "2 2017-07-31 09:45:00-04:00  2475.75  2475.75  2474.50  2474.75  NaN   \n",
       "3 2017-07-31 09:50:00-04:00  2474.50  2475.00  2473.50  2473.75  NaN   \n",
       "4 2017-07-31 09:55:00-04:00  2474.00  2474.25  2472.75  2472.75  NaN   \n",
       "\n",
       "   sin_time  cos_time  \n",
       "0 -0.402747 -0.915311  \n",
       "1 -0.422618 -0.906308  \n",
       "2 -0.442289 -0.896873  \n",
       "3 -0.461749 -0.887011  \n",
       "4 -0.480989 -0.876727  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add column for number of seconds elapsed in trading day\n",
    "five_min_data['sec'] = (five_min_data['date'].values \n",
    "                        - five_min_data['date'].values.astype('datetime64[D]')) / np.timedelta64(1,'s')\n",
    "\n",
    "#Calculate sin & cos time\n",
    "#24hr time is a cyclical continuous feature\n",
    "seconds_in_day = 24*60*60\n",
    "five_min_data['sin_time'] = np.sin(2*np.pi*five_min_data['sec']/seconds_in_day)\n",
    "five_min_data['cos_time'] = np.cos(2*np.pi*five_min_data['sec']/seconds_in_day)\n",
    "\n",
    "five_min_data.drop('sec', axis=1, inplace=True)\n",
    "five_min_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test / Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = five_min_data[81:]\n",
    "\n",
    "openp = data['open'].tolist()\n",
    "highp = data['high'].tolist()\n",
    "lowp = data['low'].tolist()\n",
    "closep = data['close'].tolist()\n",
    "emap = data['ema'].tolist()\n",
    "sin_time = data['sin_time'].tolist()\n",
    "cos_time = data['cos_time'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 64 #Number of bars in a trading day\n",
    "EMB_SIZE = 7\n",
    "STEP = 1\n",
    "FORECAST = 1\n",
    "\n",
    "X, Y = [], []\n",
    "for i in range(0, len(data), STEP):\n",
    "    try:\n",
    "        o = openp[i:i+WINDOW]\n",
    "        h = highp[i:i+WINDOW]\n",
    "        l = lowp[i:i+WINDOW]\n",
    "        c = closep[i:i+WINDOW]\n",
    "        e = emap[i:i+WINDOW]\n",
    "        ct = cos_time[i:i+WINDOW]\n",
    "        st = sin_time[i:i+WINDOW]\n",
    "\n",
    "        #o = (np.array(o) - np.mean(o)) / np.std(o)\n",
    "        #h = (np.array(h) - np.mean(h)) / np.std(h)\n",
    "        #l = (np.array(l) - np.mean(l)) / np.std(l)\n",
    "        #c = (np.array(c) - np.mean(c)) / np.std(c)\n",
    "        #e = (np.array(e) - np.mean(e)) / np.std(e)\n",
    "        \n",
    "        o = np.array(o) / c[-1] \n",
    "        h = np.array(h) / c[-1]\n",
    "        l = np.array(l) / c[-1]\n",
    "        e = np.array(e) / c[-1]\n",
    "        c = np.array(c) / c[-1]\n",
    "\n",
    "        x_i = closep[i:i+WINDOW]\n",
    "        y_i = closep[(i+WINDOW-1)+FORECAST]  \n",
    "\n",
    "        last_close = x_i[-1]\n",
    "        next_close = y_i\n",
    "\n",
    "        if last_close >= next_close:\n",
    "            y_i = [1, 0]\n",
    "        else:\n",
    "            y_i = [0, 1] \n",
    "        \n",
    "        x_i = np.column_stack((o, h, l, c, e, ct, st))\n",
    "\n",
    "    except Exception as e:\n",
    "        #e.throw()\n",
    "        break\n",
    "\n",
    "    X.append(x_i)\n",
    "    Y.append(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.92938772e-01,  9.93589148e-01,  9.92938772e-01,\n",
       "          9.93403326e-01,  9.95877377e-01,  7.51839807e-01,\n",
       "         -6.59345815e-01],\n",
       "        [ 9.98606337e-01,  9.98699247e-01,  9.96933940e-01,\n",
       "          9.97026851e-01,  9.95986851e-01, -7.79884483e-01,\n",
       "         -6.25923472e-01],\n",
       "        [ 9.97026851e-01,  9.98234693e-01,  9.96655208e-01,\n",
       "          9.98141782e-01,  9.96192083e-01, -7.66044443e-01,\n",
       "         -6.42787610e-01],\n",
       "        [ 9.98141782e-01,  9.98606337e-01,  9.98048871e-01,\n",
       "          9.98141782e-01,  9.96377768e-01, -7.51839807e-01,\n",
       "         -6.59345815e-01],\n",
       "        [ 9.98234693e-01,  9.98234693e-01,  9.97491406e-01,\n",
       "          9.97677228e-01,  9.96501526e-01, -7.37277337e-01,\n",
       "         -6.75590208e-01],\n",
       "        [ 9.97584317e-01,  9.97955960e-01,  9.97305584e-01,\n",
       "          9.97770138e-01,  9.96622346e-01, -7.22363962e-01,\n",
       "         -6.91513056e-01],\n",
       "        [ 9.97770138e-01,  9.98885069e-01,  9.97491406e-01,\n",
       "          9.98792158e-01,  9.96828995e-01, -7.07106781e-01,\n",
       "         -7.07106781e-01],\n",
       "        [ 9.98792158e-01,  9.99721267e-01,  9.98699247e-01,\n",
       "          9.99256713e-01,  9.97060206e-01, -6.91513056e-01,\n",
       "         -7.22363962e-01],\n",
       "        [ 9.99256713e-01,  9.99907089e-01,  9.99256713e-01,\n",
       "          9.99814178e-01,  9.97322489e-01, -6.75590208e-01,\n",
       "         -7.37277337e-01],\n",
       "        [ 9.99721267e-01,  9.99814178e-01,  9.99442535e-01,\n",
       "          9.99442535e-01,  9.97524399e-01, -6.59345815e-01,\n",
       "         -7.51839807e-01],\n",
       "        [ 9.99535446e-01,  1.00000000e+00,  9.99442535e-01,\n",
       "          9.99721267e-01,  9.97733624e-01, -6.42787610e-01,\n",
       "         -7.66044443e-01],\n",
       "        [ 9.99814178e-01,  1.00000000e+00,  9.99628356e-01,\n",
       "          9.99721267e-01,  9.97922923e-01, -6.25923472e-01,\n",
       "         -7.79884483e-01],\n",
       "        [ 9.99814178e-01,  1.00046455e+00,  9.99814178e-01,\n",
       "          1.00046455e+00,  9.98164984e-01, -6.08761429e-01,\n",
       "         -7.93353340e-01],\n",
       "        [ 1.00046455e+00,  1.00055747e+00,  1.00000000e+00,\n",
       "          1.00027873e+00,  9.98366293e-01, -5.91309648e-01,\n",
       "         -8.06444604e-01],\n",
       "        [ 1.00027873e+00,  1.00055747e+00,  9.98699247e-01,\n",
       "          9.99628356e-01,  9.98486490e-01, -5.73576436e-01,\n",
       "         -8.19152044e-01],\n",
       "        [ 9.99721267e-01,  1.00000000e+00,  9.99349624e-01,\n",
       "          9.99721267e-01,  9.98604087e-01, -5.55570233e-01,\n",
       "         -8.31469612e-01],\n",
       "        [ 9.99721267e-01,  1.00000000e+00,  9.99628356e-01,\n",
       "          9.99907089e-01,  9.98728183e-01, -5.37299608e-01,\n",
       "         -8.43391446e-01],\n",
       "        [ 9.99907089e-01,  1.00009291e+00,  9.99814178e-01,\n",
       "          1.00009291e+00,  9.98858157e-01, -5.18773258e-01,\n",
       "         -8.54911871e-01],\n",
       "        [ 1.00009291e+00,  1.00009291e+00,  9.99814178e-01,\n",
       "          1.00000000e+00,  9.98966904e-01, -5.00000000e-01,\n",
       "         -8.66025404e-01],\n",
       "        [ 9.99907089e-01,  1.00009291e+00,  9.99814178e-01,\n",
       "          1.00000000e+00,  9.99065294e-01, -4.80988769e-01,\n",
       "         -8.76726756e-01],\n",
       "        [ 1.00000000e+00,  1.00009291e+00,  9.99814178e-01,\n",
       "          1.00009291e+00,  9.99163162e-01, -4.61748613e-01,\n",
       "         -8.87010833e-01],\n",
       "        [ 1.00009291e+00,  1.00018582e+00,  9.99907089e-01,\n",
       "          1.00009291e+00,  9.99251710e-01, -4.42288690e-01,\n",
       "         -8.96872742e-01],\n",
       "        [ 1.00009291e+00,  1.00046455e+00,  9.99907089e-01,\n",
       "          1.00027873e+00,  9.99349521e-01, -4.22618262e-01,\n",
       "         -9.06307787e-01],\n",
       "        [ 1.00018582e+00,  1.00018582e+00,  9.99442535e-01,\n",
       "          9.99721267e-01,  9.99384926e-01, -4.02746690e-01,\n",
       "         -9.15311479e-01],\n",
       "        [ 9.99628356e-01,  1.00000000e+00,  9.99349624e-01,\n",
       "          9.99535446e-01,  9.99399261e-01, -3.82683432e-01,\n",
       "         -9.23879533e-01],\n",
       "        [ 9.99535446e-01,  9.99628356e-01,  9.99256713e-01,\n",
       "          9.99442535e-01,  9.99403382e-01, -3.62438038e-01,\n",
       "         -9.32007869e-01],\n",
       "        [ 9.99442535e-01,  9.99442535e-01,  9.98977980e-01,\n",
       "          9.99163802e-01,  9.99380565e-01, -3.42020143e-01,\n",
       "         -9.39692621e-01],\n",
       "        [ 9.99163802e-01,  9.99163802e-01,  9.98885069e-01,\n",
       "          9.99070891e-01,  9.99351072e-01, -3.21439465e-01,\n",
       "         -9.46930129e-01],\n",
       "        [ 9.99070891e-01,  9.99442535e-01,  9.98885069e-01,\n",
       "          9.99349624e-01,  9.99350934e-01, -3.00705800e-01,\n",
       "         -9.53716951e-01],\n",
       "        [ 9.99349624e-01,  9.99535446e-01,  9.99163802e-01,\n",
       "          9.99535446e-01,  9.99368507e-01, -2.79829014e-01,\n",
       "         -9.60049854e-01],\n",
       "        [ 9.99535446e-01,  9.99535446e-01,  9.99349624e-01,\n",
       "          9.99442535e-01,  9.99375557e-01, -2.58819045e-01,\n",
       "         -9.65925826e-01],\n",
       "        [ 9.99442535e-01,  9.99907089e-01,  9.99442535e-01,\n",
       "          9.99814178e-01,  9.99417331e-01, -2.37685892e-01,\n",
       "         -9.71342070e-01],\n",
       "        [ 9.99907089e-01,  1.00000000e+00,  9.99814178e-01,\n",
       "          9.99907089e-01,  9.99463974e-01, -2.16439614e-01,\n",
       "         -9.76296007e-01],\n",
       "        [ 9.99907089e-01,  1.00000000e+00,  9.99628356e-01,\n",
       "          9.99721267e-01,  9.99488478e-01, -1.95090322e-01,\n",
       "         -9.80785280e-01],\n",
       "        [ 9.99721267e-01,  1.00000000e+00,  9.99442535e-01,\n",
       "          9.99907089e-01,  9.99528346e-01, -1.73648178e-01,\n",
       "         -9.84807753e-01],\n",
       "        [ 9.99907089e-01,  9.99907089e-01,  9.99721267e-01,\n",
       "          9.99814178e-01,  9.99555568e-01, -1.52123386e-01,\n",
       "         -9.88361510e-01],\n",
       "        [ 9.99721267e-01,  9.99814178e-01,  9.99628356e-01,\n",
       "          9.99721267e-01,  9.99571349e-01, -1.30526192e-01,\n",
       "         -9.91444861e-01],\n",
       "        [ 9.99814178e-01,  9.99907089e-01,  9.99628356e-01,\n",
       "          9.99814178e-01,  9.99594476e-01, -1.08866875e-01,\n",
       "         -9.94056338e-01],\n",
       "        [ 9.99814178e-01,  9.99907089e-01,  9.99442535e-01,\n",
       "          9.99628356e-01,  9.99597702e-01, -8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [ 9.99721267e-01,  9.99721267e-01,  9.99163802e-01,\n",
       "          9.99256713e-01,  9.99565227e-01, -6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [ 9.99349624e-01,  9.99349624e-01,  9.99163802e-01,\n",
       "          9.99256713e-01,  9.99535845e-01, -4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [ 9.99256713e-01,  9.99442535e-01,  9.99163802e-01,\n",
       "          9.99256713e-01,  9.99509261e-01, -2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [ 9.99256713e-01,  9.99442535e-01,  9.99163802e-01,\n",
       "          9.99442535e-01,  9.99502906e-01, -1.83697020e-16,\n",
       "         -1.00000000e+00],\n",
       "        [ 9.99442535e-01,  9.99721267e-01,  9.99349624e-01,\n",
       "          9.99628356e-01,  9.99514854e-01,  2.18148850e-02,\n",
       "         -9.99762027e-01],\n",
       "        [ 9.99721267e-01,  9.99907089e-01,  9.99628356e-01,\n",
       "          9.99907089e-01,  9.99552209e-01,  4.36193874e-02,\n",
       "         -9.99048222e-01],\n",
       "        [ 9.99907089e-01,  1.00009291e+00,  9.99814178e-01,\n",
       "          9.99907089e-01,  9.99586007e-01,  6.54031292e-02,\n",
       "         -9.97858923e-01],\n",
       "        [ 1.00000000e+00,  1.00009291e+00,  9.99628356e-01,\n",
       "          9.99814178e-01,  9.99607738e-01,  8.71557427e-02,\n",
       "         -9.96194698e-01],\n",
       "        [ 9.99814178e-01,  1.00000000e+00,  9.99814178e-01,\n",
       "          9.99814178e-01,  9.99627399e-01,  1.08866875e-01,\n",
       "         -9.94056338e-01],\n",
       "        [ 9.99814178e-01,  1.00000000e+00,  9.99721267e-01,\n",
       "          9.99907089e-01,  9.99654036e-01,  1.30526192e-01,\n",
       "         -9.91444861e-01],\n",
       "        [ 1.00000000e+00,  1.00009291e+00,  9.99814178e-01,\n",
       "          1.00009291e+00,  9.99695834e-01,  1.52123386e-01,\n",
       "         -9.88361510e-01],\n",
       "        [ 1.00009291e+00,  1.00018582e+00,  9.99907089e-01,\n",
       "          1.00000000e+00,  9.99724802e-01,  1.73648178e-01,\n",
       "         -9.84807753e-01],\n",
       "        [ 1.00000000e+00,  1.00018582e+00,  1.00000000e+00,\n",
       "          1.00009291e+00,  9.99759860e-01,  1.95090322e-01,\n",
       "         -9.80785280e-01],\n",
       "        [ 1.00018582e+00,  1.00018582e+00,  9.99907089e-01,\n",
       "          9.99907089e-01,  9.99773882e-01,  2.16439614e-01,\n",
       "         -9.76296007e-01],\n",
       "        [ 1.00000000e+00,  1.00000000e+00,  9.99628356e-01,\n",
       "          9.99628356e-01,  9.99760022e-01,  2.37685892e-01,\n",
       "         -9.71342070e-01],\n",
       "        [ 9.99628356e-01,  1.00000000e+00,  9.99535446e-01,\n",
       "          1.00000000e+00,  9.99782877e-01,  2.58819045e-01,\n",
       "         -9.65925826e-01],\n",
       "        [ 9.99907089e-01,  1.00027873e+00,  9.99814178e-01,\n",
       "          1.00018582e+00,  9.99821253e-01,  2.79829014e-01,\n",
       "         -9.60049854e-01],\n",
       "        [ 1.00018582e+00,  1.00027873e+00,  9.99814178e-01,\n",
       "          9.99814178e-01,  9.99820579e-01,  3.00705800e-01,\n",
       "         -9.53716951e-01],\n",
       "        [ 9.99907089e-01,  1.00000000e+00,  9.99814178e-01,\n",
       "          9.99907089e-01,  9.99828818e-01,  3.21439465e-01,\n",
       "         -9.46930129e-01],\n",
       "        [ 9.99907089e-01,  1.00009291e+00,  9.99814178e-01,\n",
       "          9.99907089e-01,  9.99836273e-01,  3.42020143e-01,\n",
       "         -9.39692621e-01],\n",
       "        [ 1.00000000e+00,  1.00000000e+00,  9.99628356e-01,\n",
       "          9.99907089e-01,  9.99843017e-01,  3.62438038e-01,\n",
       "         -9.32007869e-01],\n",
       "        [ 9.99907089e-01,  1.00000000e+00,  9.99721267e-01,\n",
       "          1.00000000e+00,  9.99857968e-01,  3.82683432e-01,\n",
       "         -9.23879533e-01],\n",
       "        [ 1.00000000e+00,  1.00018582e+00,  9.99907089e-01,\n",
       "          1.00000000e+00,  9.99871495e-01,  4.02746690e-01,\n",
       "         -9.15311479e-01],\n",
       "        [ 1.00000000e+00,  1.00009291e+00,  9.99814178e-01,\n",
       "          1.00000000e+00,  9.99883733e-01,  4.22618262e-01,\n",
       "         -9.06307787e-01],\n",
       "        [ 9.99907089e-01,  1.00027873e+00,  9.99907089e-01,\n",
       "          1.00000000e+00,  9.99894806e-01,  4.42288690e-01,\n",
       "         -8.96872742e-01]]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split into train and test sets\n",
    "# Train Set will be from 8/1/17 through 12/31/17, Test Set 1/1/17 - 1/25/17\n",
    "p = 8547 #Manual split for now\n",
    "#p=8448\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "X_train = X[0:p]\n",
    "Y_train = Y[0:p]\n",
    "X_test = X[p:]\n",
    "Y_test = Y[p:]\n",
    "\n",
    "#We may want to shuffle the training data -- will look into this later\n",
    "def shuffle_in_unison(a, b):\n",
    "    # courtsey http://stackoverflow.com/users/190280/josh-bleecher-snyder\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "#X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
    "\n",
    "# Not sure why this is needed, but we apply it anyway\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1], EMB_SIZE))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1], EMB_SIZE))\n",
    "X_train[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_37 (TimeDis (None, None, 270, 480, 32 3168      \n",
      "=================================================================\n",
      "Total params: 3,168\n",
      "Trainable params: 3,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        Conv2D(32, (7, 7), padding='same', strides=2),\n",
    "        input_shape=(None, 540, 960, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_25 (TimeDis (None, None, 64, 16)      464       \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, None, 64, 16)      64        \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, None, 64, 16)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, None, 64, 16)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, None, 64, 8)       520       \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, None, 64, 8)       32        \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, None, 64, 8)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, None, 64, 8)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, None, 32)          16416     \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 26,010\n",
      "Trainable params: 25,898\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Convolution1D(filters=16,\n",
    "                                        kernel_size=4,\n",
    "                                        padding='same'),\n",
    "                          input_shape = (None, WINDOW, EMB_SIZE)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(LeakyReLU()))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "model.add(TimeDistributed(Convolution1D(filters=8,\n",
    "                        kernel_size=4,\n",
    "                        padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(LeakyReLU()))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(TimeDistributed((Dense(32))))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(LeakyReLU()))\n",
    "\n",
    "model.add(LSTM(32, dropout=0, stateful=False))\n",
    "model.add(Dropout(0))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_65 (Conv1D)           (None, 64, 16)            464       \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 64, 16)            64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)   (None, 64, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 64, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 64, 8)             520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 64, 8)             32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_98 (LeakyReLU)   (None, 64, 8)             0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 64, 8)             0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 17,690\n",
      "Trainable params: 17,578\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (WINDOW, EMB_SIZE),\n",
    "                        filters=16,\n",
    "                        kernel_size=4,\n",
    "                        padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "#model.add(MaxPooling1D(strides=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution1D(filters=8,\n",
    "                        kernel_size=4,\n",
    "                        padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "#model.add(MaxPooling1D(strides=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8547 samples, validate on 1313 samples\n",
      "Epoch 1/100\n",
      "8547/8547 [==============================] - 7s 803us/step - loss: 0.6914 - acc: 0.5243 - val_loss: 0.6922 - val_acc: 0.5225\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69224, saving model to model.hdf5\n",
      "Epoch 2/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6907 - acc: 0.5318 - val_loss: 0.6926 - val_acc: 0.5179\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69224\n",
      "Epoch 3/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6896 - acc: 0.5424 - val_loss: 0.6928 - val_acc: 0.5225\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69224\n",
      "Epoch 4/100\n",
      "8547/8547 [==============================] - 1s 147us/step - loss: 0.6894 - acc: 0.5383 - val_loss: 0.6927 - val_acc: 0.5225\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69224\n",
      "Epoch 5/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6879 - acc: 0.5510 - val_loss: 0.6928 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69224\n",
      "Epoch 6/100\n",
      "8547/8547 [==============================] - 1s 147us/step - loss: 0.6885 - acc: 0.5529 - val_loss: 0.6931 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69224\n",
      "Epoch 7/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6882 - acc: 0.5514 - val_loss: 0.6927 - val_acc: 0.5225\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69224\n",
      "Epoch 8/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6885 - acc: 0.5543 - val_loss: 0.6926 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69224\n",
      "Epoch 9/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6872 - acc: 0.5533 - val_loss: 0.6929 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69224\n",
      "Epoch 10/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6880 - acc: 0.5505 - val_loss: 0.6927 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69224\n",
      "Epoch 11/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6876 - acc: 0.5536 - val_loss: 0.6928 - val_acc: 0.5179\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69224\n",
      "Epoch 12/100\n",
      "8547/8547 [==============================] - 1s 147us/step - loss: 0.6867 - acc: 0.5565 - val_loss: 0.6926 - val_acc: 0.5156\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69224\n",
      "Epoch 13/100\n",
      "8547/8547 [==============================] - 1s 154us/step - loss: 0.6877 - acc: 0.5507 - val_loss: 0.6928 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69224\n",
      "Epoch 14/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6864 - acc: 0.5538 - val_loss: 0.6935 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69224\n",
      "Epoch 15/100\n",
      "8547/8547 [==============================] - 1s 151us/step - loss: 0.6866 - acc: 0.5518 - val_loss: 0.6933 - val_acc: 0.5232\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69224\n",
      "Epoch 16/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6870 - acc: 0.5522 - val_loss: 0.6929 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69224\n",
      "Epoch 17/100\n",
      "8547/8547 [==============================] - 1s 150us/step - loss: 0.6868 - acc: 0.5529 - val_loss: 0.6926 - val_acc: 0.5156\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.69224\n",
      "Epoch 18/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6859 - acc: 0.5566 - val_loss: 0.6927 - val_acc: 0.5149\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.69224\n",
      "Epoch 19/100\n",
      "8547/8547 [==============================] - 1s 169us/step - loss: 0.6866 - acc: 0.5554 - val_loss: 0.6926 - val_acc: 0.5149\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69224\n",
      "Epoch 20/100\n",
      "8547/8547 [==============================] - 1s 171us/step - loss: 0.6864 - acc: 0.5562 - val_loss: 0.6927 - val_acc: 0.5149\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.69224\n",
      "Epoch 21/100\n",
      "8547/8547 [==============================] - 1s 163us/step - loss: 0.6874 - acc: 0.5556 - val_loss: 0.6926 - val_acc: 0.5149\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.69224\n",
      "Epoch 22/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6861 - acc: 0.5550 - val_loss: 0.6924 - val_acc: 0.5149\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.69224\n",
      "Epoch 23/100\n",
      "8547/8547 [==============================] - 1s 159us/step - loss: 0.6869 - acc: 0.5545 - val_loss: 0.6921 - val_acc: 0.5156\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.69224 to 0.69214, saving model to model.hdf5\n",
      "Epoch 24/100\n",
      "8547/8547 [==============================] - 1s 157us/step - loss: 0.6858 - acc: 0.5552 - val_loss: 0.6924 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.69214\n",
      "Epoch 25/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6865 - acc: 0.5511 - val_loss: 0.6924 - val_acc: 0.5141\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.69214\n",
      "Epoch 26/100\n",
      "8547/8547 [==============================] - 1s 157us/step - loss: 0.6864 - acc: 0.5558 - val_loss: 0.6922 - val_acc: 0.5171\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.69214\n",
      "Epoch 27/100\n",
      "8547/8547 [==============================] - 1s 162us/step - loss: 0.6870 - acc: 0.5518 - val_loss: 0.6920 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.69214 to 0.69203, saving model to model.hdf5\n",
      "Epoch 28/100\n",
      "8547/8547 [==============================] - 1s 162us/step - loss: 0.6856 - acc: 0.5555 - val_loss: 0.6921 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.69203\n",
      "Epoch 29/100\n",
      "8547/8547 [==============================] - 1s 159us/step - loss: 0.6857 - acc: 0.5565 - val_loss: 0.6923 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.69203\n",
      "Epoch 30/100\n",
      "8547/8547 [==============================] - 1s 156us/step - loss: 0.6868 - acc: 0.5549 - val_loss: 0.6925 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.69203\n",
      "Epoch 31/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6862 - acc: 0.5547 - val_loss: 0.6923 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.69203\n",
      "Epoch 32/100\n",
      "8547/8547 [==============================] - 1s 150us/step - loss: 0.6854 - acc: 0.5547 - val_loss: 0.6923 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.69203\n",
      "Epoch 33/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6862 - acc: 0.5561 - val_loss: 0.6923 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.69203\n",
      "Epoch 34/100\n",
      "8547/8547 [==============================] - 1s 151us/step - loss: 0.6864 - acc: 0.5572 - val_loss: 0.6923 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.69203\n",
      "Epoch 35/100\n",
      "8547/8547 [==============================] - 1s 153us/step - loss: 0.6857 - acc: 0.5559 - val_loss: 0.6920 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.69203\n",
      "Epoch 36/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6855 - acc: 0.5553 - val_loss: 0.6921 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.69203\n",
      "Epoch 37/100\n",
      "8547/8547 [==============================] - 1s 151us/step - loss: 0.6853 - acc: 0.5572 - val_loss: 0.6922 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.69203\n",
      "Epoch 38/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6861 - acc: 0.5575 - val_loss: 0.6920 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.69203\n",
      "Epoch 39/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6858 - acc: 0.5555 - val_loss: 0.6918 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.69203 to 0.69176, saving model to model.hdf5\n",
      "Epoch 40/100\n",
      "8547/8547 [==============================] - 1s 159us/step - loss: 0.6855 - acc: 0.5540 - val_loss: 0.6918 - val_acc: 0.5232\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.69176\n",
      "Epoch 41/100\n",
      "8547/8547 [==============================] - 1s 158us/step - loss: 0.6862 - acc: 0.5550 - val_loss: 0.6917 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.69176 to 0.69174, saving model to model.hdf5\n",
      "Epoch 42/100\n",
      "8547/8547 [==============================] - 1s 158us/step - loss: 0.6861 - acc: 0.5583 - val_loss: 0.6916 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.69174 to 0.69160, saving model to model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "8547/8547 [==============================] - 1s 155us/step - loss: 0.6859 - acc: 0.5545 - val_loss: 0.6915 - val_acc: 0.5232\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.69160 to 0.69149, saving model to model.hdf5\n",
      "Epoch 44/100\n",
      "8547/8547 [==============================] - 1s 148us/step - loss: 0.6864 - acc: 0.5545 - val_loss: 0.6913 - val_acc: 0.5240\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.69149 to 0.69133, saving model to model.hdf5\n",
      "Epoch 45/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6860 - acc: 0.5559 - val_loss: 0.6914 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.69133\n",
      "Epoch 46/100\n",
      "8547/8547 [==============================] - 1s 155us/step - loss: 0.6856 - acc: 0.5552 - val_loss: 0.6918 - val_acc: 0.5240\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.69133\n",
      "Epoch 47/100\n",
      "8547/8547 [==============================] - 1s 155us/step - loss: 0.6852 - acc: 0.5560 - val_loss: 0.6914 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.69133\n",
      "Epoch 48/100\n",
      "8547/8547 [==============================] - 1s 153us/step - loss: 0.6857 - acc: 0.5579 - val_loss: 0.6915 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.69133\n",
      "Epoch 49/100\n",
      "8547/8547 [==============================] - 1s 147us/step - loss: 0.6862 - acc: 0.5545 - val_loss: 0.6915 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.69133\n",
      "Epoch 50/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6855 - acc: 0.5573 - val_loss: 0.6918 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.69133\n",
      "Epoch 51/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6851 - acc: 0.5552 - val_loss: 0.6916 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.69133\n",
      "Epoch 52/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6853 - acc: 0.5556 - val_loss: 0.6915 - val_acc: 0.5293\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.69133\n",
      "Epoch 53/100\n",
      "8547/8547 [==============================] - 1s 150us/step - loss: 0.6858 - acc: 0.5563 - val_loss: 0.6916 - val_acc: 0.5347\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.69133\n",
      "Epoch 54/100\n",
      "8547/8547 [==============================] - 1s 151us/step - loss: 0.6855 - acc: 0.5584 - val_loss: 0.6917 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.69133\n",
      "Epoch 55/100\n",
      "8547/8547 [==============================] - 1s 151us/step - loss: 0.6857 - acc: 0.5575 - val_loss: 0.6917 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.69133\n",
      "Epoch 56/100\n",
      "8547/8547 [==============================] - 1s 151us/step - loss: 0.6853 - acc: 0.5565 - val_loss: 0.6915 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.69133\n",
      "Epoch 57/100\n",
      "8547/8547 [==============================] - 1s 150us/step - loss: 0.6851 - acc: 0.5554 - val_loss: 0.6915 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.69133\n",
      "Epoch 58/100\n",
      "8547/8547 [==============================] - 1s 148us/step - loss: 0.6857 - acc: 0.5560 - val_loss: 0.6916 - val_acc: 0.5270\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.69133\n",
      "Epoch 59/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6855 - acc: 0.5568 - val_loss: 0.6918 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.69133\n",
      "Epoch 60/100\n",
      "8547/8547 [==============================] - 1s 152us/step - loss: 0.6859 - acc: 0.5573 - val_loss: 0.6917 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.69133\n",
      "Epoch 61/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6853 - acc: 0.5567 - val_loss: 0.6917 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.69133\n",
      "Epoch 62/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6857 - acc: 0.5545 - val_loss: 0.6917 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.69133\n",
      "Epoch 63/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6853 - acc: 0.5514 - val_loss: 0.6915 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.69133\n",
      "Epoch 64/100\n",
      "8547/8547 [==============================] - 1s 140us/step - loss: 0.6855 - acc: 0.5580 - val_loss: 0.6915 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.69133\n",
      "Epoch 65/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6853 - acc: 0.5538 - val_loss: 0.6915 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.69133\n",
      "Epoch 66/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6850 - acc: 0.5570 - val_loss: 0.6916 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.69133\n",
      "Epoch 67/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6852 - acc: 0.5562 - val_loss: 0.6914 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.69133\n",
      "Epoch 68/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6848 - acc: 0.5577 - val_loss: 0.6913 - val_acc: 0.5354\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.69133 to 0.69131, saving model to model.hdf5\n",
      "Epoch 69/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6845 - acc: 0.5562 - val_loss: 0.6911 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.69131 to 0.69115, saving model to model.hdf5\n",
      "Epoch 70/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6852 - acc: 0.5542 - val_loss: 0.6913 - val_acc: 0.5369\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.69115\n",
      "Epoch 71/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6851 - acc: 0.5583 - val_loss: 0.6914 - val_acc: 0.5377\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.69115\n",
      "Epoch 72/100\n",
      "8547/8547 [==============================] - 1s 149us/step - loss: 0.6851 - acc: 0.5541 - val_loss: 0.6916 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.69115\n",
      "Epoch 73/100\n",
      "8547/8547 [==============================] - 1s 160us/step - loss: 0.6857 - acc: 0.5562 - val_loss: 0.6916 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.69115\n",
      "Epoch 74/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6851 - acc: 0.5550 - val_loss: 0.6917 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.69115\n",
      "Epoch 75/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6846 - acc: 0.5570 - val_loss: 0.6918 - val_acc: 0.5331\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.69115\n",
      "Epoch 76/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6852 - acc: 0.5528 - val_loss: 0.6917 - val_acc: 0.5362\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.69115\n",
      "Epoch 77/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6847 - acc: 0.5574 - val_loss: 0.6916 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.69115\n",
      "Epoch 78/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6845 - acc: 0.5574 - val_loss: 0.6917 - val_acc: 0.5308\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.69115\n",
      "Epoch 79/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6848 - acc: 0.5588 - val_loss: 0.6917 - val_acc: 0.5263\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.69115\n",
      "Epoch 80/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6847 - acc: 0.5552 - val_loss: 0.6917 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.69115\n",
      "Epoch 81/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6853 - acc: 0.5547 - val_loss: 0.6918 - val_acc: 0.5316\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.69115\n",
      "Epoch 82/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6851 - acc: 0.5567 - val_loss: 0.6919 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.69115\n",
      "Epoch 83/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6851 - acc: 0.5547 - val_loss: 0.6917 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.69115\n",
      "Epoch 84/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6855 - acc: 0.5563 - val_loss: 0.6918 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.69115\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8547/8547 [==============================] - 1s 141us/step - loss: 0.6848 - acc: 0.5554 - val_loss: 0.6918 - val_acc: 0.5270\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.69115\n",
      "Epoch 86/100\n",
      "8547/8547 [==============================] - 1s 140us/step - loss: 0.6849 - acc: 0.5570 - val_loss: 0.6917 - val_acc: 0.5270\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.69115\n",
      "Epoch 87/100\n",
      "8547/8547 [==============================] - 1s 142us/step - loss: 0.6842 - acc: 0.5573 - val_loss: 0.6918 - val_acc: 0.5263\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.69115\n",
      "Epoch 88/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6849 - acc: 0.5559 - val_loss: 0.6917 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.69115\n",
      "Epoch 89/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6846 - acc: 0.5575 - val_loss: 0.6918 - val_acc: 0.5263\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.69115\n",
      "Epoch 90/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6845 - acc: 0.5550 - val_loss: 0.6918 - val_acc: 0.5286\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.69115\n",
      "Epoch 91/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6852 - acc: 0.5554 - val_loss: 0.6920 - val_acc: 0.5263\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.69115\n",
      "Epoch 92/100\n",
      "8547/8547 [==============================] - 1s 144us/step - loss: 0.6854 - acc: 0.5574 - val_loss: 0.6919 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.69115\n",
      "Epoch 93/100\n",
      "8547/8547 [==============================] - 1s 146us/step - loss: 0.6846 - acc: 0.5561 - val_loss: 0.6919 - val_acc: 0.5270\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.69115\n",
      "Epoch 94/100\n",
      "8547/8547 [==============================] - 1s 142us/step - loss: 0.6843 - acc: 0.5601 - val_loss: 0.6925 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.69115\n",
      "Epoch 95/100\n",
      "8547/8547 [==============================] - 1s 140us/step - loss: 0.6845 - acc: 0.5587 - val_loss: 0.6923 - val_acc: 0.5270\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.69115\n",
      "Epoch 96/100\n",
      "8547/8547 [==============================] - 1s 141us/step - loss: 0.6846 - acc: 0.5597 - val_loss: 0.6927 - val_acc: 0.5194\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.69115\n",
      "Epoch 97/100\n",
      "8547/8547 [==============================] - 1s 145us/step - loss: 0.6840 - acc: 0.5616 - val_loss: 0.6925 - val_acc: 0.5179\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.69115\n",
      "Epoch 98/100\n",
      "8547/8547 [==============================] - 1s 142us/step - loss: 0.6845 - acc: 0.5601 - val_loss: 0.6926 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.69115\n",
      "Epoch 99/100\n",
      "8547/8547 [==============================] - 1s 142us/step - loss: 0.6849 - acc: 0.5570 - val_loss: 0.6930 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.69115\n",
      "Epoch 100/100\n",
      "8547/8547 [==============================] - 1s 143us/step - loss: 0.6847 - acc: 0.5591 - val_loss: 0.6929 - val_acc: 0.5232\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.69115\n"
     ]
    }
   ],
   "source": [
    "opt = Nadam(lr=0.0001)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=30, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, \n",
    "          epochs = 100, \n",
    "          batch_size = 128, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[reduce_lr, checkpointer],\n",
    "          shuffle='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.load_weights(\"model.hdf5\")\n",
    "pred = model.predict(np.array(X_test), batch_size=128)\n",
    "\n",
    "C = confusion_matrix([np.argmax(y) for y in Y_test], [np.argmax(y) for y in pred])\n",
    "\n",
    "print (C / C.astype(np.float).sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5830708 , 0.41692916], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>0.585188</td>\n",
       "      <td>0.414812</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.585003</td>\n",
       "      <td>0.414997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.584993</td>\n",
       "      <td>0.415007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.584852</td>\n",
       "      <td>0.415148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.584818</td>\n",
       "      <td>0.415182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.584813</td>\n",
       "      <td>0.415187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>0.584811</td>\n",
       "      <td>0.415189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>0.584795</td>\n",
       "      <td>0.415205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>0.584754</td>\n",
       "      <td>0.415246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>0.584741</td>\n",
       "      <td>0.415259</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.584736</td>\n",
       "      <td>0.415264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>0.584735</td>\n",
       "      <td>0.415265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.584713</td>\n",
       "      <td>0.415287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0.584699</td>\n",
       "      <td>0.415301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0.584642</td>\n",
       "      <td>0.415358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>0.584551</td>\n",
       "      <td>0.415449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>0.577206</td>\n",
       "      <td>0.422794</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>0.576575</td>\n",
       "      <td>0.423425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.423682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.576310</td>\n",
       "      <td>0.423690</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1    2    3\n",
       "1198  0.585188  0.414812  1.0  0.0\n",
       "874   0.585003  0.414997  1.0  0.0\n",
       "469   0.584993  0.415007  1.0  0.0\n",
       "307   0.584852  0.415148  1.0  0.0\n",
       "388   0.584818  0.415182  1.0  0.0\n",
       "145   0.584813  0.415187  1.0  0.0\n",
       "955   0.584811  0.415189  1.0  0.0\n",
       "550   0.584795  0.415205  0.0  1.0\n",
       "631   0.584754  0.415246  0.0  1.0\n",
       "712   0.584741  0.415259  1.0  0.0\n",
       "226   0.584736  0.415264  1.0  0.0\n",
       "1117  0.584735  0.415265  1.0  0.0\n",
       "64    0.584713  0.415287  0.0  1.0\n",
       "1279  0.584699  0.415301  0.0  1.0\n",
       "793   0.584642  0.415358  0.0  1.0\n",
       "1036  0.584551  0.415449  1.0  0.0\n",
       "804   0.577206  0.422794  1.0  0.0\n",
       "642   0.576575  0.423425  1.0  0.0\n",
       "1047  0.576318  0.423682  0.0  1.0\n",
       "237   0.576310  0.423690  1.0  0.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.concatenate((pred, Y_test), axis=1))\n",
    "df[df[0]>.6].head()\n",
    "df.sort_values(1,axis=0).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[812,   0],\n",
       "       [501,   0]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83673469, 0.15855573, 0.00470958],\n",
       "       [0.78915663, 0.20481928, 0.0060241 ],\n",
       "       [0.86335404, 0.13043478, 0.00621118]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C / C.astype(np.float).sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55539956, 0.44460044])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = Y_train.sum(axis=0) / Y_train.shape[0]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4436597 , 0.5563404 ],\n",
       "       [0.5040323 , 0.49596766],\n",
       "       [0.5359964 , 0.4640036 ],\n",
       "       ...,\n",
       "       [0.57311577, 0.4268842 ],\n",
       "       [0.5362818 , 0.46371824],\n",
       "       [0.5139088 , 0.48609126]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.random.binomial(1, probs[1], pred.shape[0])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58421851 0.41578149]\n",
      " [0.59026688 0.40973312]]\n"
     ]
    }
   ],
   "source": [
    "C1 = confusion_matrix([np.argmax(y) for y in Y_test], s)\n",
    "print (C1 / C1.astype(np.float).sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4984567901234568"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([np.argmax(y) for y in Y_test] == s).sum() / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f61cf415860>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
